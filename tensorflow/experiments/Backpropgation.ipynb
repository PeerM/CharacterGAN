{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Through Single *Sampled* Logit.\n",
    "We test the ability to backpropagate through a single logit.  If you only allow gradients through a single logit, then at a given training iteration, you will only impact one column of your weight matrix, that is, weights contributing to the value of that logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 0.595851\n",
      "Sample: [3]\n",
      "[[ 0.83569515  0.13769221  0.77196145  0.58878052  0.30522072]\n",
      " [ 0.78347039  0.50974977  0.25244844  0.84535742  0.30451775]\n",
      " [ 0.25584829  0.79547215  0.14775825  0.79268515  0.19243336]]\n",
      "Loss at iteration 1: 0.079502\n",
      "Sample: [1]\n",
      "[[ 0.73569524  0.13769221  0.77196145  0.58878052  0.30522072]\n",
      " [ 0.68347049  0.50974977  0.25244844  0.84535742  0.30451775]\n",
      " [ 0.15584838  0.79547215  0.14775825  0.79268515  0.19243336]]\n",
      "Loss at iteration 2: 0.266778\n",
      "Sample: [1]\n",
      "[[ 0.66868973  0.21210557  0.77196145  0.58878052  0.30522072]\n",
      " [ 0.61646497  0.58416313  0.25244844  0.84535742  0.30451775]\n",
      " [ 0.08884285  0.8698855   0.14775825  0.79268515  0.19243336]]\n",
      "Loss at iteration 3: 0.266778\n",
      "Sample: [1]\n",
      "[[ 0.61689448  0.26962709  0.83584219  0.58878052  0.30522072]\n",
      " [ 0.56466973  0.64168465  0.31632921  0.84535742  0.30451775]\n",
      " [ 0.03704758  0.92740703  0.21163902  0.79268515  0.19243336]]\n",
      "Loss at iteration 4: 0.212614\n",
      "Sample: [1]\n",
      "[[ 0.57446676  0.34147394  0.88816965  0.58878052  0.30522072]\n",
      " [ 0.52224201  0.71353149  0.36865667  0.84535742  0.30451775]\n",
      " [-0.00538013  0.99925387  0.26396647  0.79268515  0.19243336]]\n",
      "Loss at iteration 5: 0.188512\n",
      "Sample: [4]\n",
      "[[ 0.59110361  0.40220094  0.93239832  0.58878052  0.30522072]\n",
      " [ 0.53887886  0.77425849  0.41288531  0.84535742  0.30451775]\n",
      " [ 0.01125675  1.05998087  0.30819511  0.79268515  0.19243336]]\n",
      "Loss at iteration 6: 0.173716\n",
      "Sample: [3]\n",
      "[[ 0.63169283  0.45453972  0.97051764  0.58878052  0.30522072]\n",
      " [ 0.57946807  0.82659727  0.45100465  0.84535742  0.30451775]\n",
      " [ 0.05184598  1.11231959  0.34631446  0.79268515  0.19243336]]\n",
      "Loss at iteration 7: 0.595851\n",
      "Sample: [2]\n",
      "[[ 0.66713965  0.50996774  1.00380754  0.58878052  0.30522072]\n",
      " [ 0.61491489  0.8820253   0.4842945   0.84535742  0.30451775]\n",
      " [ 0.08729282  1.16774762  0.37960431  0.79268515  0.19243336]]\n",
      "Loss at iteration 8: 0.160733\n",
      "Sample: [3]\n",
      "[[ 0.69838816  0.55883086  1.03315461  0.53920525  0.30522072]\n",
      " [ 0.6461634   0.93088841  0.51364154  0.79578215  0.30451775]\n",
      " [ 0.11854135  1.21661079  0.40895134  0.74310988  0.19243336]]\n",
      "Loss at iteration 9: 0.413280\n",
      "Sample: [2]\n",
      "[[ 0.72612858  0.60306787  1.05920696  0.49519554  0.30522072]\n",
      " [ 0.67390382  0.97512543  0.53969389  0.75177246  0.30451775]\n",
      " [ 0.14628176  1.26084781  0.43500373  0.6991002   0.19243336]]\n",
      "Loss at iteration 10: 0.469055\n",
      "Sample: [1]\n",
      "[[ 0.75088632  0.63932115  1.08245814  0.45591784  0.30522072]\n",
      " [ 0.69866157  1.01137865  0.56294507  0.71249479  0.30451775]\n",
      " [ 0.17103949  1.29710102  0.4582549   0.65982252  0.19243336]]\n",
      "Loss at iteration 11: 0.523317\n",
      "Sample: [1]\n",
      "[[ 0.77307391  0.66518825  1.10329556  0.42071763  0.30522072]\n",
      " [ 0.72084916  1.03724575  0.58378249  0.67729461  0.30451775]\n",
      " [ 0.19322707  1.32296813  0.47909233  0.62462234  0.19243336]]\n",
      "Loss at iteration 12: 0.175221\n",
      "Sample: [1]\n",
      "[[ 0.79302353  0.68844628  1.12203121  0.38906792  0.35336357]\n",
      " [ 0.74079877  1.06050372  0.60251814  0.6456449   0.3526606 ]\n",
      " [ 0.21317668  1.3462261   0.49782798  0.59297264  0.24057622]]\n",
      "Loss at iteration 13: 0.625453\n",
      "Sample: [2]\n",
      "[[ 0.81100816  0.70941347  1.1389215   0.38030311  0.39676449]\n",
      " [ 0.7587834   1.08147085  0.61940837  0.6368801   0.39606151]\n",
      " [ 0.23116131  1.36719322  0.51471823  0.58420783  0.28397712]]\n",
      "Loss at iteration 14: 0.182525\n",
      "Sample: [3]\n",
      "[[ 0.82725579  0.71549529  1.15418041  0.37238485  0.43597364]\n",
      " [ 0.77503103  1.08755267  0.63466734  0.6289618   0.43527067]\n",
      " [ 0.24740894  1.37327504  0.5299772   0.57628953  0.32318628]]\n",
      "Loss at iteration 15: 0.534957\n",
      "Sample: [1]\n",
      "[[ 0.84195954  0.72099918  1.10757339  0.36521897  0.47145709]\n",
      " [ 0.78973478  1.09305656  0.58806032  0.62179595  0.47075412]\n",
      " [ 0.26211271  1.37877893  0.48337018  0.56912369  0.35866973]]\n",
      "Loss at iteration 16: 0.152817\n",
      "Sample: [3]\n",
      "[[ 0.85528487  0.71287292  1.06533563  0.35872489  0.50361407]\n",
      " [ 0.80306011  1.0849303   0.54582262  0.61530185  0.50291109]\n",
      " [ 0.27543804  1.37065268  0.44113249  0.56262958  0.39082667]]\n",
      "Loss at iteration 17: 0.176535\n",
      "Sample: [1]\n",
      "[[ 0.8673749   0.69374377  1.02701354  0.35283282  0.53278995]\n",
      " [ 0.81515014  1.06580114  0.50750053  0.60940981  0.53208697]\n",
      " [ 0.28752804  1.35152352  0.40281039  0.55673754  0.42000255]]\n",
      "Loss at iteration 18: 0.582901\n",
      "Sample: [1]\n",
      "[[ 0.87835443  0.66648519  0.9922114   0.34748197  0.55928594]\n",
      " [ 0.82612967  1.03854251  0.47269839  0.60405898  0.55858296]\n",
      " [ 0.29850754  1.32426488  0.36800826  0.55138671  0.44649854]]\n",
      "Loss at iteration 19: 0.584069\n",
      "Sample: [2]\n",
      "[[ 0.88833308  0.64171135  0.94094801  0.34261888  0.58336669]\n",
      " [ 0.83610833  1.01376867  0.421435    0.5991959   0.58266371]\n",
      " [ 0.30848622  1.29949105  0.31674486  0.54652363  0.4705793 ]]\n",
      "Loss at iteration 20: 0.162684\n",
      "Sample: [0]\n",
      "[[ 0.89740777  0.61326224  0.89432865  0.33819634  0.60526592]\n",
      " [ 0.84518301  0.98531955  0.37481561  0.59477335  0.60456294]\n",
      " [ 0.31756091  1.27104199  0.27012548  0.54210109  0.49247855]]\n",
      "Loss at iteration 21: 0.184960\n",
      "Sample: [2]\n",
      "[[ 0.9056645   0.58737743  0.84229183  0.33417243  0.62519127]\n",
      " [ 0.85343975  0.95943475  0.32277879  0.59074944  0.62448829]\n",
      " [ 0.32581764  1.24515712  0.21808866  0.53807718  0.51240391]]\n",
      "Loss at iteration 22: 0.160663\n",
      "Sample: [0]\n",
      "[[ 0.8770082   0.56381637  0.79492652  0.33050975  0.64332783]\n",
      " [ 0.82478344  0.93587369  0.27541345  0.5870868   0.64262486]\n",
      " [ 0.29716134  1.221596    0.17072332  0.53441453  0.53054047]]\n",
      "Loss at iteration 23: 0.148291\n",
      "Sample: [0]\n",
      "[[ 0.85091627  0.5423637   0.75179982  0.32717484  0.62409633]\n",
      " [ 0.79869151  0.91442102  0.23228672  0.58375186  0.62339336]\n",
      " [ 0.27106941  1.20014334  0.12759659  0.53107959  0.51130897]]\n",
      "Loss at iteration 24: 0.178663\n",
      "Sample: [2]\n",
      "[[ 0.82715356  0.52282614  0.71252298  0.32413763  0.58364725]\n",
      " [ 0.77492881  0.89488345  0.19300988  0.58071464  0.58294427]\n",
      " [ 0.24730669  1.18060577  0.08831975  0.52804238  0.47085986]]\n",
      "Loss at iteration 25: 0.164687\n",
      "Sample: [1]\n",
      "[[ 0.80550826  0.5050295   0.67674595  0.32137105  0.53208542]\n",
      " [ 0.7532835   0.87708682  0.15723285  0.57794809  0.53138244]\n",
      " [ 0.22566138  1.16280913  0.05254272  0.52527583  0.41929805]]\n",
      "Loss at iteration 26: 0.581504\n",
      "Sample: [4]\n",
      "[[ 0.78578919  0.48881659  0.64415276  0.3536303   0.48511219]\n",
      " [ 0.73356444  0.86087388  0.12463968  0.61020738  0.48440921]\n",
      " [ 0.20594232  1.14659619  0.01994954  0.55753511  0.37232482]]\n",
      "Loss at iteration 27: 0.169617\n",
      "Sample: [4]\n",
      "[[ 0.76782352  0.47404531  0.61445773  0.40347984  0.44231576]\n",
      " [ 0.71559876  0.8461026   0.09494463  0.66005695  0.44161278]\n",
      " [ 0.18797664  1.13182485 -0.0097455   0.60738468  0.32952839]]\n",
      "Loss at iteration 28: 0.083157\n",
      "Sample: [1]\n",
      "[[ 0.75145465  0.46058691  0.58740199  0.44889879  0.41411257]\n",
      " [ 0.6992299   0.83264416  0.06788889  0.70547587  0.41340959]\n",
      " [ 0.17160776  1.11836648 -0.03680125  0.6528036   0.3013252 ]]\n",
      "Loss at iteration 29: 0.573771\n",
      "Sample: [1]\n",
      "[[ 0.7365405   0.44832456  0.5627507   0.49028131  0.40449342]\n",
      " [ 0.68431574  0.82038182  0.0432376   0.74685836  0.40379044]\n",
      " [ 0.15669359  1.10610414 -0.06145254  0.69418609  0.29170606]]\n",
      "Loss at iteration 30: 0.577262\n",
      "Sample: [3]\n",
      "[[ 0.72295201  0.43715218  0.54029059  0.51576436  0.3957293 ]\n",
      " [ 0.67072725  0.80920947  0.02077751  0.77234143  0.39502633]\n",
      " [ 0.1431051   1.09493172 -0.08391263  0.71966916  0.28294194]]\n",
      "Loss at iteration 31: 0.180819\n",
      "Sample: [2]\n",
      "[[  7.08418727e-01   4.26973313e-01   5.19827783e-01   5.38981318e-01\n",
      "    3.87744546e-01]\n",
      " [  6.56193972e-01   7.99030602e-01   3.14716250e-04   7.95558393e-01\n",
      "    3.87041569e-01]\n",
      " [  1.28571838e-01   1.08475280e+00  -1.04375422e-01   7.42886126e-01\n",
      "    2.74957180e-01]]\n",
      "Loss at iteration 32: 0.183554\n",
      "Sample: [1]\n",
      "[[ 0.69517869  0.4177002   0.50118583  0.53586799  0.38047031]\n",
      " [ 0.64295393  0.78975749 -0.01832725  0.79244506  0.37976733]\n",
      " [ 0.11533178  1.07547963 -0.12301739  0.7397728   0.26768294]]\n",
      "Loss at iteration 33: 0.186196\n",
      "Sample: [1]\n",
      "[[ 0.68653059  0.40925291  0.48420402  0.53303194  0.37384388]\n",
      " [ 0.63430583  0.7813102  -0.03530905  0.78960901  0.3731409 ]\n",
      " [ 0.10668369  1.06703234 -0.13999918  0.73693675  0.26105651]]\n",
      "Loss at iteration 34: 0.188728\n",
      "Sample: [3]\n",
      "[[ 0.67865342  0.40155861  0.46873599  0.5102393   0.36780813]\n",
      " [ 0.62642866  0.7736159  -0.05077709  0.76681638  0.36710516]\n",
      " [ 0.0988065   1.05933797 -0.15546721  0.71414411  0.25502077]]\n",
      "Loss at iteration 35: 0.185720\n",
      "Sample: [1]\n",
      "[[ 0.67147917  0.41074169  0.45464826  0.48948061  0.36231098]\n",
      " [ 0.61925441  0.78279895 -0.06486481  0.74605769  0.361608  ]\n",
      " [ 0.09163224  1.06852102 -0.16955493  0.69338542  0.24952362]]\n",
      "Loss at iteration 36: 0.194395\n",
      "Sample: [0]\n",
      "[[ 0.66494584  0.43363795  0.44181916  0.47057652  0.35730496]\n",
      " [ 0.61272109  0.80569518 -0.07769391  0.7271536   0.35660198]\n",
      " [ 0.08509894  1.09141731 -0.18238404  0.67448133  0.24451761]]\n",
      "Loss at iteration 37: 0.199813\n",
      "Sample: [1]\n",
      "[[ 0.658997    0.45448601  0.43013769  0.44745645  0.35274675]\n",
      " [ 0.60677224  0.82654327 -0.08937538  0.70403355  0.35204378]\n",
      " [ 0.07915007  1.11226535 -0.19406553  0.65136129  0.2399594 ]]\n",
      "Loss at iteration 38: 0.089965\n",
      "Sample: [1]\n",
      "[[ 0.65358102  0.48394549  0.41950256  0.42640734  0.34859684]\n",
      " [ 0.60135627  0.85600275 -0.10001051  0.68298441  0.34789386]\n",
      " [ 0.07373407  1.14172482 -0.20470065  0.63031214  0.23580949]]\n",
      "Loss at iteration 39: 0.163987\n",
      "Sample: [1]\n",
      "[[ 0.64865083  0.51076251  0.40982139  0.41201949  0.34481916]\n",
      " [ 0.59642607  0.88281977 -0.10969169  0.66859657  0.34411618]\n",
      " [ 0.06880386  1.16854191 -0.21438183  0.6159243   0.23203181]]\n",
      "Loss at iteration 40: 0.161752\n",
      "Sample: [3]\n",
      "[[ 0.64416349  0.53517073  0.40100983  0.39892402  0.37026983]\n",
      " [ 0.59193873  0.90722799 -0.11850326  0.65550113  0.36956686]\n",
      " [ 0.06431651  1.19295013 -0.22319341  0.60282886  0.2574825 ]]\n",
      "Loss at iteration 41: 0.089445\n",
      "Sample: [2]\n",
      "[[ 0.65480679  0.5573833   0.39299092  0.38700655  0.3934311 ]\n",
      " [ 0.60258204  0.92944056 -0.12652218  0.64358366  0.39272812]\n",
      " [ 0.07495981  1.21516275 -0.23121233  0.59091139  0.28064376]]\n",
      "Loss at iteration 42: 0.160751\n",
      "Sample: [1]\n",
      "[[ 0.6644913   0.57867235  0.38569441  0.37616271  0.41450581]\n",
      " [ 0.61226654  0.95072961 -0.1338187   0.63273984  0.41380283]\n",
      " [ 0.08464429  1.23645175 -0.23850885  0.58006757  0.30171847]]\n",
      "Loss at iteration 43: 0.578912\n",
      "Sample: [0]\n",
      "[[ 0.68352509  0.59804064  0.37905622  0.36629722  0.43367913]\n",
      " [ 0.63130033  0.9700979  -0.14045691  0.62287438  0.43297616]\n",
      " [ 0.10367806  1.25582004 -0.24514706  0.57020211  0.3208918 ]]\n",
      "Loss at iteration 44: 0.162628\n",
      "Sample: [3]\n",
      "[[ 0.70083904  0.61264002  0.37301782  0.35732314  0.45112002]\n",
      " [ 0.64861429  0.98469728 -0.14649533  0.6139003   0.45041704]\n",
      " [ 0.12099202  1.27041936 -0.25118548  0.56122804  0.33833268]]\n",
      "Loss at iteration 45: 0.077517\n",
      "Sample: [0]\n",
      "[[ 0.71658623  0.62137681  0.36752585  0.34916115  0.46698266]\n",
      " [ 0.66436148  0.99343407 -0.15198731  0.60573828  0.46627969]\n",
      " [ 0.13673919  1.27915609 -0.25667745  0.55306602  0.35419533]]\n",
      "Loss at iteration 46: 0.163759\n",
      "Sample: [0]\n",
      "[[ 0.73090631  0.62385714  0.36253157  0.34173885  0.48140773]\n",
      " [ 0.67868155  0.9959144  -0.15698157  0.59831595  0.48070475]\n",
      " [ 0.15105926  1.28163636 -0.26167172  0.54564369  0.3686204 ]]\n",
      "Loss at iteration 47: 0.081184\n",
      "Sample: [1]\n",
      "[[ 0.7439267   0.62611234  0.35799059  0.3349902   0.49736685]\n",
      " [ 0.69170195  0.9981696  -0.16152255  0.59156728  0.49666387]\n",
      " [ 0.16407962  1.28389156 -0.2662127   0.53889501  0.38457951]]\n",
      "Loss at iteration 48: 0.084473\n",
      "Sample: [0]\n",
      "[[ 0.75576359  0.62816256  0.35386238  0.35676262  0.51187539]\n",
      " [ 0.70353884  1.00021982 -0.16565078  0.61333972  0.51117241]\n",
      " [ 0.17591651  1.28594172 -0.27034092  0.56066746  0.39908803]]\n",
      "Loss at iteration 49: 0.163744\n",
      "Sample: [1]\n",
      "[[ 0.766523    0.62372196  0.35010993  0.37655318  0.52506328]\n",
      " [ 0.71429825  0.99577922 -0.16940324  0.63313025  0.5243603 ]\n",
      " [ 0.18667594  1.28150117 -0.27409336  0.58045799  0.41227588]]\n",
      "[[ 1.07039201  0.34384429  1.22959161]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 5\n",
    "output_dim = 3\n",
    "lr = 1e-1\n",
    "num_iterations = 50\n",
    "print_every = 1\n",
    "\n",
    "x = tf.fill([1, input_dim], 1.)\n",
    "y = tf.fill([1, output_dim], 1.)\n",
    "\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n",
    "    logits = tf.matmul(x, W_gen)\n",
    "\n",
    "    # Sample through single logit.\n",
    "    categorical = Categorical(logits)\n",
    "    index = tf.stop_gradient(categorical.sample())\n",
    "#     index = tf.stop_gradient(Categorical(logits).sample()\n",
    "#     index = tf.squeeze(sample_op)\n",
    "#     index = tf.constant(0, dtype=tf.int32)\n",
    "    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n",
    "    logits = logits * one_hot\n",
    "\n",
    "    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n",
    "    output = tf.matmul(logits, W_dis)\n",
    "\n",
    "        \n",
    "with tf.name_scope('Loss'):\n",
    "    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_vars = [W_gen]\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in xrange(num_iterations):\n",
    "        if i % print_every == 0:\n",
    "            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n",
    "            print('Sample: [%d]' % sess.run(index))\n",
    "            print sess.run(W_gen)\n",
    "        sess.run(train_op)\n",
    "    print sess.run(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Training, even for the most trivial task imaginable, is very volatile.  This does not appear to be a plausible way to proceed.\n",
    "\n",
    "### TensorFlow Bug\n",
    "Mathematical impossibility. For a drawn sample $i$, columns $\\neq i$ of the generator weight matrix $W_{gen}$ are updating! This only occurs when I'm using Categorical.  This is filed as [Issue 4074](https://github.com/tensorflow/tensorflow/issues/4074).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution vs. One-Hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: 0.800210\n",
      "One-hot: 0.799524\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "num_samples = 250\n",
    "\n",
    "x = tf.constant([[0.2, 0.3, 0.5]], dtype = tf.float32) \n",
    "\n",
    "with tf.name_scope('dist_input'):\n",
    "    W_dis = tf.Variable(tf.random_uniform([input_dim, output_dim]), name = 'W_dis')\n",
    "    output = tf.matmul(x, W_dis)\n",
    "\n",
    "\n",
    "with tf.name_scope('one_hot_input'):\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    outputs = []\n",
    "    \n",
    "    for i in xrange(num_samples):\n",
    "        sample_op = Categorical(x).sample(n=1)\n",
    "        index = tf.squeeze(sample_op)\n",
    "        x = tf.one_hot(index, input_dim, dtype = tf.float32)\n",
    "        x = tf.reshape(x, [1, input_dim])\n",
    "        outputs.append(tf.matmul(x, W_dis))\n",
    "    \n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    print('Distribution: %f' %  sess.run(tf.squeeze(output)))\n",
    "    print('One-hot: %f' % np.average(sess.run(outputs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
