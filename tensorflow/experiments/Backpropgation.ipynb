{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Through Single *Sampled* Logit.\n",
    "We test the ability to backpropagate through a single logit.  If you only allow gradients through a single logit, then at a given training iteration, you will only impact one column of your weight matrix, that is, weights contributing to the value of that logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 5\n",
    "output_dim = 3\n",
    "lr = 1e-1\n",
    "num_iterations = 50\n",
    "print_every = 1\n",
    "\n",
    "x = tf.fill([1, input_dim], 1.)\n",
    "y = tf.fill([1, output_dim], 1.)\n",
    "\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n",
    "    logits = tf.matmul(x, W_gen)\n",
    "\n",
    "    # Sample through single logit.\n",
    "    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n",
    "    index = tf.squeeze(sample_op)\n",
    "#     index = tf.constant(0, dtype=tf.int32)\n",
    "    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n",
    "    logits = logits * one_hot\n",
    "\n",
    "    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n",
    "    output = tf.matmul(logits, W_dis)\n",
    "\n",
    "        \n",
    "with tf.name_scope('Loss'):\n",
    "    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_vars = [W_gen]\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in xrange(num_iterations):\n",
    "        if i % print_every == 0:\n",
    "            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n",
    "            print('Sample: [%d]' % sess.run(index))\n",
    "            print sess.run(W_gen)\n",
    "        sess.run(train_op)\n",
    "    print sess.run(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Training, even for the most trivial task imaginable, is very volatile.  This does not appear to be a plausible way to proceed.\n",
    "\n",
    "### TensorFlow Bug\n",
    "Mathematical impossibility. For a drawn sample $i$, columns $\\neq i$ of the generator weight matrix $W_{gen}$ are updating! This only occurs when I'm using Categorical.  This is filed as [Issue 4074](https://github.com/tensorflow/tensorflow/issues/4074).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution vs. One-Hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: 0.800210\n",
      "One-hot: 0.799524\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "num_samples = 250\n",
    "\n",
    "x = tf.constant([[0.2, 0.3, 0.5]], dtype = tf.float32) \n",
    "\n",
    "with tf.name_scope('dist_input'):\n",
    "    W_dis = tf.Variable(tf.random_uniform([input_dim, output_dim]), name = 'W_dis')\n",
    "    output = tf.matmul(x, W_dis)\n",
    "\n",
    "\n",
    "with tf.name_scope('one_hot_input'):\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    outputs = []\n",
    "    \n",
    "    for i in xrange(num_samples):\n",
    "        sample_op = Categorical(x).sample(n=1)\n",
    "        index = tf.squeeze(sample_op)\n",
    "        x = tf.one_hot(index, input_dim, dtype = tf.float32)\n",
    "        x = tf.reshape(x, [1, input_dim])\n",
    "        outputs.append(tf.matmul(x, W_dis))\n",
    "    \n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    print('Distribution: %f' %  sess.run(tf.squeeze(output)))\n",
    "    print('One-hot: %f' % np.average(sess.run(outputs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
