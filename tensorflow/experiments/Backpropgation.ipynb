{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Through Single *Sampled* Logit.\n",
    "We test the ability to backpropagate through a single logit.  If you only allow gradients through a single logit, then at a given training iteration, you will only impact one column of your weight matrix, that is, weights contributing to the value of that logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 0.149865\n",
      "Sample: [1]\n",
      "[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]\n",
      " [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]\n",
      " [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]\n",
      "Loss at iteration 1: 0.149865\n",
      "Sample: [1]\n",
      "[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]\n",
      " [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]\n",
      " [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]\n",
      "Loss at iteration 2: 0.181932\n",
      "Sample: [0]\n",
      "[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]\n",
      " [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]\n",
      " [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]\n",
      "Loss at iteration 3: 0.431700\n",
      "Sample: [3]\n",
      "[[ 0.81418228  0.64673406  0.29257485  0.55864608  0.28714925]\n",
      " [ 0.26089203  0.47155327  0.77213979  0.02805829  0.93370825]\n",
      " [ 0.56620026  0.30244702  0.36692584  0.59907818  0.37625915]]\n",
      "Loss at iteration 4: 0.023184\n",
      "Sample: [1]\n",
      "[[ 0.81418228  0.59733492  0.33969316  0.55864608  0.23482172]\n",
      " [ 0.26089203  0.42215413  0.81925809  0.02805829  0.88138074]\n",
      " [ 0.56620026  0.25304788  0.41404414  0.59907818  0.32393163]]\n",
      "Loss at iteration 5: 0.158787\n",
      "Sample: [3]\n",
      "[[ 0.81418228  0.55558133  0.3795189   0.5040977   0.190593  ]\n",
      " [ 0.26089203  0.38040057  0.85908383 -0.02649011  0.837152  ]\n",
      " [ 0.56620026  0.21129432  0.45386988  0.5445298   0.2797029 ]]\n",
      "Loss at iteration 6: 0.183241\n",
      "Sample: [0]\n",
      "[[ 0.81418228  0.51959521  0.35261357  0.45708409  0.1524736 ]\n",
      " [ 0.26089203  0.34441444  0.83217853 -0.07350373  0.79903263]\n",
      " [ 0.56620026  0.1753082   0.42696455  0.49751619  0.2415835 ]]\n",
      "Loss at iteration 7: 0.214591\n",
      "Sample: [2]\n",
      "[[ 0.7635448   0.4881683   0.329117    0.4160268   0.1191837 ]\n",
      " [ 0.21025454  0.31298754  0.80868196 -0.11456101  0.76574272]\n",
      " [ 0.51556277  0.14388129  0.40346798  0.4564589   0.20829359]]\n",
      "Loss at iteration 8: 0.200608\n",
      "Sample: [3]\n",
      "[[ 0.71890479  0.48574674  0.30840334  0.37983233  0.08983663]\n",
      " [ 0.16561452  0.31056598  0.78796834 -0.15075549  0.73639566]\n",
      " [ 0.47092277  0.14145975  0.38275433  0.42026442  0.17894652]]\n",
      "Loss at iteration 9: 0.191409\n",
      "Sample: [2]\n",
      "[[ 0.67927629  0.50418127  0.2900151   0.34770122  0.0637842 ]\n",
      " [ 0.12598599  0.3290005   0.76958013 -0.1828866   0.71034324]\n",
      " [ 0.43129426  0.15989427  0.36436608  0.38813332  0.15289411]]\n",
      "Loss at iteration 10: 0.432897\n",
      "Sample: [4]\n",
      "[[ 0.69870996  0.5206337   0.27360401  0.31902492  0.04053298]\n",
      " [ 0.1454197   0.3454529   0.753169   -0.21156292  0.68709201]\n",
      " [ 0.45072797  0.17634669  0.34795499  0.35945702  0.12964289]]\n",
      "Loss at iteration 11: 0.428647\n",
      "Sample: [3]\n",
      "[[ 0.7161262   0.53537816  0.25889656  0.29332554  0.06501611]\n",
      " [ 0.16283596  0.36019737  0.73846155 -0.23726228  0.71157515]\n",
      " [ 0.46814424  0.19109115  0.33324754  0.33375764  0.15412602]]\n",
      "Loss at iteration 12: 0.162622\n",
      "Sample: [1]\n",
      "[[ 0.74982822  0.54863542  0.2456726   0.27021834  0.08702973]\n",
      " [ 0.19653799  0.37345463  0.72523761 -0.26036948  0.73358876]\n",
      " [ 0.50184625  0.2043484   0.32002357  0.31065044  0.17613964]]\n",
      "Loss at iteration 13: 0.425892\n",
      "Sample: [4]\n",
      "[[ 0.78410554  0.56058687  0.23375116  0.24938715  0.10687506]\n",
      " [ 0.23081529  0.38540608  0.71331614 -0.28120068  0.75343412]\n",
      " [ 0.53612357  0.21629985  0.30810213  0.28981924  0.19598497]]\n",
      "Loss at iteration 14: 0.429016\n",
      "Sample: [4]\n",
      "[[ 0.80289859  0.57138401  0.22298113  0.23056787  0.12480369]\n",
      " [ 0.24960835  0.39620322  0.70254612 -0.30001995  0.77136278]\n",
      " [ 0.55491662  0.22709699  0.29733211  0.27099997  0.2139136 ]]\n",
      "Loss at iteration 15: 0.433294\n",
      "Sample: [4]\n",
      "[[ 0.81990594  0.58115518  0.21947502  0.21353683  0.14102872]\n",
      " [ 0.26661569  0.40597442  0.69904    -0.31705099  0.78758782]\n",
      " [ 0.57192397  0.23686817  0.29382598  0.25396892  0.23013863]]\n",
      "Loss at iteration 16: 0.438361\n",
      "Sample: [0]\n",
      "[[ 0.83531886  0.59529018  0.2162976   0.19810241  0.15573269]\n",
      " [ 0.28202862  0.42010945  0.69586259 -0.33248541  0.80229181]\n",
      " [ 0.5873369   0.25100321  0.29064855  0.23853451  0.2448426 ]]\n",
      "Loss at iteration 17: 0.443938\n",
      "Sample: [4]\n",
      "[[ 0.84930295  0.60811484  0.22165681  0.18409882  0.16907354]\n",
      " [ 0.2960127   0.43293411  0.70122182 -0.34648898  0.81563264]\n",
      " [ 0.60132098  0.26382786  0.29600775  0.22453092  0.25818345]]\n",
      "Loss at iteration 18: 0.143380\n",
      "Sample: [0]\n",
      "[[ 0.82721388  0.61976153  0.22652377  0.1713815   0.181189  ]\n",
      " [ 0.27392364  0.44458079  0.70608878 -0.35920632  0.82774812]\n",
      " [ 0.57923192  0.27547455  0.30087471  0.2118136   0.27029893]]\n",
      "Loss at iteration 19: 0.064420\n",
      "Sample: [4]\n",
      "[[ 0.80713838  0.63034654  0.23094708  0.15982343  0.19911359]\n",
      " [ 0.25384811  0.4551658   0.7105121  -0.37076437  0.84567273]\n",
      " [ 0.55915642  0.28605956  0.30529803  0.20025553  0.28822351]]\n",
      "Loss at iteration 20: 0.181938\n",
      "Sample: [0]\n",
      "[[ 0.78888154  0.63746017  0.23496968  0.14931242  0.21541438]\n",
      " [ 0.23559125  0.46227944  0.7145347  -0.38127539  0.86197352]\n",
      " [ 0.54089957  0.29317319  0.30932063  0.18974452  0.30452427]]\n",
      "Loss at iteration 21: 1.123582\n",
      "Sample: [0]\n",
      "[[ 0.77227026  0.64393264  0.2386297   0.13974883  0.22950681]\n",
      " [ 0.21897998  0.46875188  0.71819472 -0.39083898  0.87606597]\n",
      " [ 0.5242883   0.29964563  0.31298065  0.18018092  0.31861672]]\n",
      "Loss at iteration 22: 0.181639\n",
      "Sample: [1]\n",
      "[[ 0.75715023  0.64982402  0.24196115  0.13104379  0.23826763]\n",
      " [ 0.20385996  0.47464326  0.72152615 -0.39954403  0.88482678]\n",
      " [ 0.50916827  0.30553702  0.3163121   0.17147589  0.32737753]]\n",
      "Loss at iteration 23: 1.214945\n",
      "Sample: [1]\n",
      "[[ 0.74392891  0.6551882   0.24499448  0.12311774  0.24624448]\n",
      " [ 0.1906386   0.48000744  0.72455949 -0.40747008  0.89280361]\n",
      " [ 0.49594691  0.31090119  0.31934541  0.16354984  0.33535436]]\n",
      "Loss at iteration 24: 0.181723\n",
      "Sample: [2]\n",
      "[[ 0.73188782  0.65322822  0.24775702  0.11589925  0.25350922]\n",
      " [ 0.17859751  0.47804743  0.72732204 -0.41468859  0.90006834]\n",
      " [ 0.48390582  0.30894119  0.32210797  0.15633135  0.34261912]]\n",
      "Loss at iteration 25: 0.073919\n",
      "Sample: [2]\n",
      "[[ 0.72091967  0.64495349  0.25027341  0.10932396  0.26012665]\n",
      " [ 0.16762936  0.46977273  0.72983843 -0.42126387  0.90668577]\n",
      " [ 0.47293767  0.30066648  0.32462436  0.14975606  0.34923655]]\n",
      "Loss at iteration 26: 0.182020\n",
      "Sample: [1]\n",
      "[[ 0.71092761  0.63231587  0.25256586  0.10333382  0.26615518]\n",
      " [ 0.15763728  0.45713508  0.73213089 -0.42725402  0.9127143 ]\n",
      " [ 0.46294558  0.28802884  0.32691681  0.14376591  0.35526508]]\n",
      "Loss at iteration 27: 0.143428\n",
      "Sample: [2]\n",
      "[[ 0.70182401  0.62080199  0.25465447  0.09787632  0.25852704]\n",
      " [ 0.14853369  0.44562116  0.73421949 -0.43271154  0.90508616]\n",
      " [ 0.45384198  0.27651492  0.32900542  0.13830841  0.34763694]]\n",
      "Loss at iteration 28: 0.143426\n",
      "Sample: [2]\n",
      "[[ 0.69352955  0.60929799  0.25655743  0.09290387  0.25157687]\n",
      " [ 0.14023921  0.43411717  0.73612249 -0.437684    0.89813602]\n",
      " [ 0.44554752  0.26501092  0.33090839  0.13333596  0.34068677]]\n",
      "Loss at iteration 29: 0.144435\n",
      "Sample: [0]\n",
      "[[ 0.68597221  0.59881634  0.25829127  0.08837333  0.23578274]\n",
      " [ 0.13268188  0.42363554  0.73785633 -0.44221455  0.88234192]\n",
      " [ 0.43799019  0.2545293   0.33264223  0.12880543  0.32489264]]\n",
      "Loss at iteration 30: 0.430967\n",
      "Sample: [0]\n",
      "[[ 0.70590216  0.58926642  0.25987101  0.0842455   0.22139253]\n",
      " [ 0.15261181  0.4140856   0.73943609 -0.44634238  0.86795169]\n",
      " [ 0.45792013  0.24497935  0.33422196  0.1246776   0.31050244]]\n",
      "Loss at iteration 31: 0.425061\n",
      "Sample: [2]\n",
      "[[ 0.74128407  0.58056569  0.26131025  0.08048475  0.20828198]\n",
      " [ 0.18799372  0.4053849   0.74087536 -0.45010313  0.85484111]\n",
      " [ 0.49330205  0.23627865  0.3356612   0.12091684  0.29739189]]\n",
      "Loss at iteration 32: 0.151165\n",
      "Sample: [4]\n",
      "[[ 0.77935565  0.57263923  0.26262143  0.07705863  0.19633804]\n",
      " [ 0.22606531  0.3974584   0.74218655 -0.45352924  0.84289718]\n",
      " [ 0.53137362  0.22835216  0.33697239  0.11749072  0.28544796]]\n",
      "Loss at iteration 33: 0.431662\n",
      "Sample: [0]\n",
      "[[ 0.81403679  0.5654186   0.26381585  0.07393762  0.18952309]\n",
      " [ 0.26074642  0.39023781  0.74338096 -0.45665023  0.83608222]\n",
      " [ 0.56605476  0.22113156  0.3381668   0.11436972  0.278633  ]]\n",
      "Loss at iteration 34: 0.442382\n",
      "Sample: [0]\n",
      "[[ 0.84562641  0.55884165  0.26490378  0.07109483  0.18915969]\n",
      " [ 0.29233605  0.38366085  0.74446893 -0.45949301  0.83571881]\n",
      " [ 0.59764439  0.21455461  0.33925474  0.11152693  0.27826959]]\n",
      "Loss at iteration 35: 0.456283\n",
      "Sample: [2]\n",
      "[[ 0.8743971   0.55285162  0.25783956  0.06850572  0.18882872]\n",
      " [ 0.32110673  0.37767079  0.7374047  -0.46208212  0.83538783]\n",
      " [ 0.62641507  0.20856456  0.33219051  0.10893781  0.27793863]]\n",
      "Loss at iteration 36: 0.066163\n",
      "Sample: [2]\n",
      "[[ 0.86131853  0.54739672  0.25140646  0.06614792  0.18852732]\n",
      " [ 0.30802813  0.3722159   0.73097157 -0.46443993  0.83508641]\n",
      " [ 0.6133365   0.20310968  0.32575741  0.10658002  0.27763724]]\n",
      "Loss at iteration 37: 0.181655\n",
      "Sample: [1]\n",
      "[[ 0.84940988  0.55549008  0.24554883  0.06400104  0.18825288]\n",
      " [ 0.29611948  0.38030925  0.72511393 -0.4665868   0.83481199]\n",
      " [ 0.60142785  0.21120305  0.31989977  0.10443313  0.27736279]]\n",
      "Loss at iteration 38: 0.439574\n",
      "Sample: [2]\n",
      "[[ 0.83856791  0.56285846  0.23933086  0.06204646  0.18800302]\n",
      " [ 0.28527755  0.38767767  0.71889597 -0.46854138  0.83456212]\n",
      " [ 0.59058589  0.21857147  0.31368181  0.10247856  0.27711293]]\n",
      "Loss at iteration 39: 0.436046\n",
      "Sample: [2]\n",
      "[[ 0.82869846  0.57956672  0.23367062  0.0602672   0.18777557]\n",
      " [ 0.27540809  0.40438595  0.71323574 -0.47032064  0.83433467]\n",
      " [ 0.58071643  0.23527975  0.30802158  0.1006993   0.27688548]]\n",
      "Loss at iteration 40: 0.065097\n",
      "Sample: [0]\n",
      "[[ 0.81971556  0.59477419  0.22851881  0.05864777  0.19424492]\n",
      " [ 0.26642516  0.41959342  0.70808393 -0.47194007  0.84080404]\n",
      " [ 0.57173353  0.25048721  0.30286977  0.09907986  0.28335485]]\n",
      "Loss at iteration 41: 0.183317\n",
      "Sample: [0]\n",
      "[[ 0.81154072  0.60861367  0.22383045  0.05717401  0.20506369]\n",
      " [ 0.2582503   0.43343291  0.70339555 -0.47341383  0.85162282]\n",
      " [ 0.5635587   0.26432669  0.29818138  0.0976061   0.29417363]]\n",
      "Loss at iteration 42: 0.184139\n",
      "Sample: [0]\n",
      "[[ 0.8041023   0.62248552  0.21956444  0.05583302  0.21490781]\n",
      " [ 0.25081187  0.44730476  0.69912952 -0.47475481  0.86146694]\n",
      " [ 0.55612028  0.27819854  0.29391539  0.09626511  0.30401775]]\n",
      "Loss at iteration 43: 0.145051\n",
      "Sample: [0]\n",
      "[[ 0.78314126  0.63510579  0.21568331  0.05461301  0.22386378]\n",
      " [ 0.2298508   0.45992506  0.69524843 -0.47597483  0.8704229 ]\n",
      " [ 0.53515923  0.29081884  0.29003426  0.0950451   0.31297374]]\n",
      "Loss at iteration 44: 0.065403\n",
      "Sample: [1]\n",
      "[[ 0.75643587  0.64658576  0.21215287  0.05350324  0.23201053]\n",
      " [ 0.20314543  0.47140503  0.69171798 -0.47708461  0.87856966]\n",
      " [ 0.50845385  0.30229881  0.28650382  0.09393533  0.3211205 ]]\n",
      "Loss at iteration 45: 0.066960\n",
      "Sample: [4]\n",
      "[[ 0.7321471   0.65702689  0.20894191  0.12040555  0.23942006]\n",
      " [ 0.17885669  0.48184615  0.68850702 -0.4101823   0.88597918]\n",
      " [ 0.4841651   0.31273994  0.28329286  0.16083765  0.32853004]]\n",
      "Loss at iteration 46: 0.154918\n",
      "Sample: [4]\n",
      "[[ 0.71005958  0.66652179  0.20602195  0.18124473  0.23826967]\n",
      " [ 0.15676914  0.49134105  0.68558705 -0.34934312  0.88482881]\n",
      " [ 0.46207756  0.32223484  0.28037289  0.22167684  0.32737964]]\n",
      "Loss at iteration 47: 0.159146\n",
      "Sample: [2]\n",
      "[[ 0.68997675  0.67515492  0.20336701  0.26635161  0.23722368]\n",
      " [ 0.13668628  0.49997419  0.68293208 -0.26423624  0.8837828 ]\n",
      " [ 0.4419947   0.33086798  0.27771795  0.30678374  0.32633367]]\n",
      "Loss at iteration 48: 0.066239\n",
      "Sample: [2]\n",
      "[[ 0.69416338  0.68300337  0.20095338  0.34372279  0.23627278]\n",
      " [ 0.1408729   0.50782263  0.68051845 -0.18686506  0.88283187]\n",
      " [ 0.4461813   0.33871639  0.27530432  0.38415492  0.32538277]]\n",
      "Loss at iteration 49: 0.159428\n",
      "Sample: [1]\n",
      "[[ 0.6979689   0.67568737  0.19875945  0.41405115  0.23540844]\n",
      " [ 0.14467841  0.50050664  0.67832452 -0.1165367   0.88196754]\n",
      " [ 0.44998682  0.33140039  0.27311039  0.45448327  0.32451841]]\n",
      "[[ 1.10990691  0.33050671  0.08560674]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 5\n",
    "output_dim = 3\n",
    "lr = 1e-1\n",
    "num_iterations = 50\n",
    "print_every = 1\n",
    "\n",
    "x = tf.fill([1, input_dim], 1.)\n",
    "y = tf.fill([1, output_dim], 1.)\n",
    "\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n",
    "    logits = tf.matmul(x, W_gen)\n",
    "\n",
    "    # Sample through single logit.\n",
    "    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n",
    "    index = tf.squeeze(sample_op)\n",
    "#     index = tf.constant(0, dtype=tf.int32)\n",
    "    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n",
    "    logits = logits * one_hot\n",
    "\n",
    "    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n",
    "    output = tf.matmul(logits, W_dis)\n",
    "\n",
    "        \n",
    "with tf.name_scope('Loss'):\n",
    "    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    train_vars = [W_gen]\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in xrange(num_iterations):\n",
    "        if i % print_every == 0:\n",
    "            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n",
    "            print('Sample: [%d]' % sess.run(index))\n",
    "            print sess.run(W_gen)\n",
    "        sess.run(train_op)\n",
    "    print sess.run(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Training, even for the most trivial task imaginable, is very volatile.  This does not appear to be a plausible way to proceed.\n",
    "\n",
    "### TensorFlow Bug\n",
    "Mathematical impossibility. It appears that for a sample $i$, that columns $\\neq i$ of the generator weight matrix $W_{gen}$ are updating! This only occurs when I'm using Categorical.  This is filed as [Issue 4074](https://github.com/tensorflow/tensorflow/issues/4074).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backpropagation Through Distirbut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
