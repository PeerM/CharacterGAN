{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Assess whether to proceed with functions or classes\n",
    "* Better way to assign learning rate\n",
    "* Better way to deal with stateful-ness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_length = 10\n",
    "rnn_size   = 128\n",
    "vocab_size = 5\n",
    "save_dir_GAN = 'models_GAN'\n",
    "vocab_file = 'simple_vocab.pkl'\n",
    "model = 'lstm'\n",
    "num_layers = 2\n",
    "grad_clip = 5\n",
    "real_input_file = 'real_reviews.txt'\n",
    "fake_input_file = 'fake_reviews.txt'\n",
    "num_epochs_dis = 1\n",
    "data_dir = 'data/gan/'\n",
    "dis_lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(save_dir_GAN, vocab_file):\n",
    "    '''Load vocabulary objects.\n",
    "    \n",
    "    Args:\n",
    "        save_dir_GAN:  Directory containing vocab_files.\n",
    "        vocab_file: Vocab file to use.\n",
    "    '''\n",
    "    with open(os.path.join(save_dir_GAN, vocab_file)) as f:\n",
    "            chars, vocab = cPickle.load(f)\n",
    "    return chars, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import rnn_cell\n",
    "from tensorflow.python.ops.nn import rnn\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops.nn import seq2seq \n",
    "# TODO: Eliminate depencence on seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def construct_gan():\n",
    "    if model == 'rnn':\n",
    "        cell_gen = rnn_cell.BasicRNNCell(rnn_size)\n",
    "        cell_dis = rnn_cell.BasicRNNCell(rnn_size)\n",
    "    elif model == 'gru':\n",
    "        cell_gen = rnn_cell.GRUCell(rnn_size)\n",
    "        cell_dis = rnn_cell.GRUCell(rnn_size)\n",
    "    elif model == 'lstm':\n",
    "        cell_gen = rnn_cell.BasicLSTMCell(rnn_size, state_is_tuple=False)\n",
    "        cell_dis = rnn_cell.BasicLSTMCell(rnn_size, state_is_tuple=False)\n",
    "    else:\n",
    "        raise NotImplementedError('Model type not supported: {}'\n",
    "                                  .format(model))\n",
    "\n",
    "    \n",
    "    # TODO: Better initialization.\n",
    "    indices = []\n",
    "    batch_indices = tf.fill([batch_size], 0)\n",
    "\n",
    "    # Targets for Generator are 1            \n",
    "    targets = tf.ones([batch_size, seq_length], dtype=tf.int32)\n",
    "                \n",
    "    # Generator Portion of GAN.\n",
    "    with tf.variable_scope('generator'):\n",
    "        outputs_gen, logit_sequence = [], []\n",
    "        cell_gen = rnn_cell.MultiRNNCell([cell_gen] * num_layers)\n",
    "        state_gen = cell_gen.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            softmax_w = tf.get_variable('softmax_w', [rnn_size, vocab_size])\n",
    "            softmax_b = tf.get_variable('softmax_b', [vocab_size])\n",
    "            embedding = tf.get_variable('embedding', [vocab_size, rnn_size])\n",
    "            inp = tf.nn.embedding_lookup(embedding, batch_indices)\n",
    "    \n",
    "            for i in xrange(seq_length):\n",
    "                indices.append(batch_indices)\n",
    "                if i > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "              \n",
    "                # RNN.\n",
    "                output_gen, state_gen = cell_gen(inp, state_gen)\n",
    "                logits_gen = tf.nn.xw_plus_b(output_gen, softmax_w, \n",
    "                                             softmax_b)\n",
    "                \n",
    "                # Sampling.\n",
    "                sample_op = tf.stop_gradient(Categorical(\n",
    "                                            logits_gen).sample(n=1))\n",
    "                batch_indices = tf.squeeze(sample_op)\n",
    "                inp = tf.nn.embedding_lookup(embedding, batch_indices)                \n",
    "                \n",
    "                # Use Only Logit Sampled.\n",
    "                one_hot = tf.stop_gradient(tf.one_hot(batch_indices,\n",
    "                                                      depth = vocab_size,\n",
    "                                                      dtype = tf.float32))\n",
    "                logit_gen = one_hot * logits_gen\n",
    "                logit_sequence.append(logit_gen)\n",
    "                outputs_gen.append(output_gen)\n",
    "\n",
    "        # Sampled indices\n",
    "        sample_op = tf.pack(indices)\n",
    "    \n",
    "    # Discriminator Portion of GAN. \n",
    "    with tf.variable_scope('discriminator'):\n",
    "        cell_dis = rnn_cell.MultiRNNCell([cell_dis] * num_layers)\n",
    "        state_dis = cell_dis.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            softmax_w_dis = tf.get_variable('softmax_w', [rnn_size, 2])\n",
    "            softmax_b_dis = tf.get_variable('softmax_b', [2])\n",
    "            embedding_dis = tf.get_variable('embedding', [vocab_size, rnn_size])            \n",
    "\n",
    "            # Input sequence to Discriminator.\n",
    "            inputs_dis = []\n",
    "            for logit in logit_sequence:\n",
    "                inputs_dis.append(tf.matmul(logit, embedding_dis))\n",
    "\n",
    "            # RNN.\n",
    "            assert len(inputs_dis) == len(outputs_gen)\n",
    "            outputs_dis, last_state_dis = seq2seq.rnn_decoder(inputs_dis,\n",
    "                state_dis, cell_dis, loop_function=None)\n",
    "\n",
    "            # Predictions.\n",
    "            probs, logits = [], []\n",
    "            for output_dis in outputs_dis:\n",
    "                logit = tf.nn.xw_plus_b(output_dis, softmax_w_dis, softmax_b_dis)\n",
    "                prob = tf.nn.softmax(logit)\n",
    "                logits.append(logit)\n",
    "                probs.append(prob)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = seq2seq.sequence_loss_by_example(logits, \n",
    "                            tf.unpack(tf.transpose(targets)), \n",
    "                            tf.unpack(tf.transpose(tf.ones_like(targets, dtype=tf.float32))))\n",
    "            cost = tf.reduce_sum(loss) / batch_size\n",
    "        return cost, probs\n",
    "        \n",
    "                \n",
    "def construct_gan_train_op(cost, lr):\n",
    "    with tf.name_scope('train'):        \n",
    "        tvars = tf.trainable_variables()\n",
    "        gen_vars = [v for v in tvars if not v.name.startswith(\"discriminator/\")]\n",
    "        gen_grads = tf.gradients(cost, gen_vars)\n",
    "        gen_grads_clipped, _ = tf.clip_by_global_norm(gen_grads, grad_clip)\n",
    "        gen_optimizer = tf.train.AdamOptimizer(lr_gen)\n",
    "        gen_train_op = gen_optimizer.apply_gradients(zip(gen_grads_clipped, gen_vars))\n",
    "     \n",
    "    return gen_train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f3f0c39bc50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f3f0c39bf10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for iteration 0: 1.386907\n",
      "Iteration 0\n",
      "[[ 0.75656724  0.24343273]\n",
      " [ 0.75585538  0.2441446 ]\n",
      " [ 0.75701225  0.24298772]\n",
      " [ 0.75585538  0.2441446 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75718009  0.24281994]\n",
      " [ 0.75718009  0.24281994]\n",
      " [ 0.75682068  0.24317932]\n",
      " [ 0.75777435  0.24222566]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75514424  0.24485575]\n",
      " [ 0.75563771  0.24436228]\n",
      " [ 0.75336301  0.24663706]\n",
      " [ 0.75618541  0.24381457]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75780493  0.24219503]\n",
      " [ 0.75257683  0.24742323]\n",
      " [ 0.75185186  0.2481482 ]\n",
      " [ 0.75514865  0.24485134]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.75301361  0.2469864 ]\n",
      " [ 0.75311083  0.24688919]\n",
      " [ 0.75149679  0.24850318]\n",
      " [ 0.7521385   0.24786156]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.75189459  0.24810544]\n",
      " [ 0.74954516  0.25045484]\n",
      " [ 0.75008762  0.24991232]\n",
      " [ 0.75377095  0.2462291 ]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74795067  0.2520493 ]\n",
      " [ 0.75002819  0.24997184]\n",
      " [ 0.75194526  0.24805474]\n",
      " [ 0.75116998  0.24882999]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74513036  0.25486964]\n",
      " [ 0.75217253  0.24782749]\n",
      " [ 0.75212419  0.24787578]\n",
      " [ 0.74288106  0.25711891]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.7449376   0.25506246]\n",
      " [ 0.74408787  0.2559121 ]\n",
      " [ 0.7448808   0.2551192 ]\n",
      " [ 0.7481516   0.25184843]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.7387591   0.26124096]\n",
      " [ 0.74081707  0.25918299]\n",
      " [ 0.74404794  0.255952  ]\n",
      " [ 0.74217236  0.25782758]] \n",
      "\n",
      "Loss for iteration 1: 1.390662\n",
      "Iteration 0\n",
      "[[ 0.75700337  0.2429966 ]\n",
      " [ 0.75648797  0.24351203]\n",
      " [ 0.75581098  0.24418896]\n",
      " [ 0.75648797  0.24351203]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75595379  0.24404624]\n",
      " [ 0.75560319  0.24439676]\n",
      " [ 0.75732899  0.24267107]\n",
      " [ 0.75452036  0.24547958]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75271571  0.24728429]\n",
      " [ 0.7554788   0.24452116]\n",
      " [ 0.75666612  0.24333389]\n",
      " [ 0.75616616  0.24383385]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75423074  0.24576926]\n",
      " [ 0.75453562  0.24546434]\n",
      " [ 0.75204509  0.24795491]\n",
      " [ 0.75690156  0.24309842]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.75139219  0.24860775]\n",
      " [ 0.75290191  0.2470981 ]\n",
      " [ 0.75317574  0.24682419]\n",
      " [ 0.75321573  0.24678427]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.75088662  0.24911335]\n",
      " [ 0.75147724  0.24852276]\n",
      " [ 0.74775815  0.25224191]\n",
      " [ 0.75063628  0.24936374]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74877554  0.25122443]\n",
      " [ 0.7540223   0.24597774]\n",
      " [ 0.74732411  0.25267586]\n",
      " [ 0.74539351  0.25460646]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74519593  0.25480407]\n",
      " [ 0.74584246  0.25415751]\n",
      " [ 0.74656153  0.2534385 ]\n",
      " [ 0.74484259  0.25515741]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74741715  0.25258282]\n",
      " [ 0.7413829   0.25861713]\n",
      " [ 0.74158818  0.25841182]\n",
      " [ 0.7493549   0.2506451 ]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73910427  0.26089576]\n",
      " [ 0.74470598  0.25529402]\n",
      " [ 0.73869628  0.26130372]\n",
      " [ 0.73857993  0.26142007]] \n",
      "\n",
      "Loss for iteration 2: 1.382859\n",
      "Iteration 0\n",
      "[[ 0.75641698  0.24358308]\n",
      " [ 0.75700015  0.24299984]\n",
      " [ 0.75641698  0.24358308]\n",
      " [ 0.75698209  0.24301794]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75618309  0.2438169 ]\n",
      " [ 0.7560823   0.24391772]\n",
      " [ 0.75334132  0.24665865]\n",
      " [ 0.75590777  0.24409226]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75439966  0.24560036]\n",
      " [ 0.75439966  0.24560036]\n",
      " [ 0.75673586  0.24326406]\n",
      " [ 0.75456834  0.24543165]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75460654  0.24539346]\n",
      " [ 0.75300282  0.24699715]\n",
      " [ 0.75240606  0.247594  ]\n",
      " [ 0.75484461  0.24515538]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.753497    0.24650303]\n",
      " [ 0.75321078  0.2467892 ]\n",
      " [ 0.75300384  0.24699615]\n",
      " [ 0.75492257  0.24507743]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.75100034  0.24899961]\n",
      " [ 0.74499166  0.25500837]\n",
      " [ 0.74989456  0.25010544]\n",
      " [ 0.74791592  0.25208408]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.75131017  0.24868988]\n",
      " [ 0.74690592  0.25309405]\n",
      " [ 0.74429387  0.25570616]\n",
      " [ 0.74708831  0.25291163]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74644643  0.25355366]\n",
      " [ 0.75019282  0.24980715]\n",
      " [ 0.74718416  0.25281581]\n",
      " [ 0.74234229  0.25765774]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74206799  0.25793201]\n",
      " [ 0.74433535  0.25566465]\n",
      " [ 0.7450999   0.25490013]\n",
      " [ 0.74479085  0.25520912]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.74276549  0.25723445]\n",
      " [ 0.73860252  0.26139745]\n",
      " [ 0.74516058  0.25483936]\n",
      " [ 0.73427922  0.26572075]] \n",
      "\n",
      "Loss for iteration 3: 1.382535\n",
      "Iteration 0\n",
      "[[ 0.75699705  0.24300294]\n",
      " [ 0.75696319  0.24303679]\n",
      " [ 0.75699705  0.24300294]\n",
      " [ 0.75634712  0.2436529 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75639987  0.24360013]\n",
      " [ 0.75646788  0.24353211]\n",
      " [ 0.7555154   0.24448465]\n",
      " [ 0.75563431  0.24436571]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75252908  0.24747092]\n",
      " [ 0.75317436  0.24682565]\n",
      " [ 0.75626469  0.24373531]\n",
      " [ 0.75499576  0.24500422]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75323725  0.24676275]\n",
      " [ 0.75214744  0.24785252]\n",
      " [ 0.75356013  0.24643984]\n",
      " [ 0.75541818  0.24458176]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.7541759   0.24582405]\n",
      " [ 0.75176096  0.24823907]\n",
      " [ 0.75080693  0.24919304]\n",
      " [ 0.74936664  0.25063333]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.7436859   0.25631413]\n",
      " [ 0.74906373  0.25093627]\n",
      " [ 0.74924362  0.25075632]\n",
      " [ 0.75066715  0.24933285]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.7462644   0.2537356 ]\n",
      " [ 0.73801202  0.26198789]\n",
      " [ 0.74954206  0.25045797]\n",
      " [ 0.7515074   0.24849257]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74172145  0.25827849]\n",
      " [ 0.74510556  0.25489444]\n",
      " [ 0.74629933  0.2537007 ]\n",
      " [ 0.74403584  0.25596422]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74198842  0.25801158]\n",
      " [ 0.7434541   0.25654587]\n",
      " [ 0.74423254  0.25576743]\n",
      " [ 0.74784052  0.25215948]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73270118  0.26729885]\n",
      " [ 0.73294157  0.2670584 ]\n",
      " [ 0.74612343  0.2538766 ]\n",
      " [ 0.73555714  0.26444283]] \n",
      "\n",
      "Loss for iteration 4: 1.382508\n",
      "Iteration 0\n",
      "[[ 0.75561744  0.24438253]\n",
      " [ 0.75694191  0.24305809]\n",
      " [ 0.75694191  0.24305809]\n",
      " [ 0.75767934  0.24232069]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75484055  0.24515946]\n",
      " [ 0.7558589   0.24414109]\n",
      " [ 0.75730002  0.2427    ]\n",
      " [ 0.75533259  0.24466746]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75251144  0.24748857]\n",
      " [ 0.75556284  0.24443714]\n",
      " [ 0.75467819  0.2453218 ]\n",
      " [ 0.75528771  0.24471234]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74742669  0.25257331]\n",
      " [ 0.75325906  0.24674091]\n",
      " [ 0.75266588  0.24733414]\n",
      " [ 0.75404418  0.24595582]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74558699  0.25441301]\n",
      " [ 0.7521463   0.24785365]\n",
      " [ 0.75120872  0.24879125]\n",
      " [ 0.752119    0.24788104]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.74908078  0.25091922]\n",
      " [ 0.74455404  0.25544599]\n",
      " [ 0.74411601  0.25588402]\n",
      " [ 0.74975938  0.25024056]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74719763  0.25280234]\n",
      " [ 0.74731183  0.2526882 ]\n",
      " [ 0.74543875  0.25456122]\n",
      " [ 0.7390644   0.2609356 ]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74099332  0.25900674]\n",
      " [ 0.7443918   0.2556082 ]\n",
      " [ 0.74242842  0.25757161]\n",
      " [ 0.74730033  0.2526997 ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74771273  0.25228724]\n",
      " [ 0.73801911  0.26198086]\n",
      " [ 0.73683345  0.26316652]\n",
      " [ 0.73961836  0.26038164]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73606765  0.26393235]\n",
      " [ 0.7337237   0.2662763 ]\n",
      " [ 0.73233807  0.26766196]\n",
      " [ 0.73689485  0.26310515]] \n",
      "\n",
      "Loss for iteration 5: 1.383072\n",
      "Iteration 0\n",
      "[[ 0.75691617  0.24308389]\n",
      " [ 0.75691617  0.24308389]\n",
      " [ 0.75698978  0.24301024]\n",
      " [ 0.75698978  0.24301024]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.7563563   0.24364366]\n",
      " [ 0.75254941  0.24745061]\n",
      " [ 0.75741822  0.24258186]\n",
      " [ 0.75551307  0.24448691]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75523746  0.2447626 ]\n",
      " [ 0.75483811  0.24516191]\n",
      " [ 0.755225    0.24477503]\n",
      " [ 0.75666291  0.24333708]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74936211  0.25063789]\n",
      " [ 0.75735897  0.24264097]\n",
      " [ 0.75235868  0.24764137]\n",
      " [ 0.75108474  0.24891527]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74579763  0.25420234]\n",
      " [ 0.75025719  0.24974272]\n",
      " [ 0.75139475  0.24860525]\n",
      " [ 0.75664467  0.24335524]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.74804252  0.25195748]\n",
      " [ 0.74975568  0.25024429]\n",
      " [ 0.75030327  0.2496967 ]\n",
      " [ 0.74711633  0.25288367]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74774408  0.25225592]\n",
      " [ 0.75099432  0.24900569]\n",
      " [ 0.74318731  0.25681269]\n",
      " [ 0.74323863  0.25676137]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74174553  0.25825444]\n",
      " [ 0.74716359  0.25283641]\n",
      " [ 0.74291259  0.25708735]\n",
      " [ 0.74090767  0.2590923 ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73786664  0.26213333]\n",
      " [ 0.73520958  0.26479045]\n",
      " [ 0.74349087  0.25650921]\n",
      " [ 0.73791152  0.26208848]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73717684  0.26282319]\n",
      " [ 0.73849279  0.26150727]\n",
      " [ 0.73330557  0.2666944 ]\n",
      " [ 0.73388731  0.26611266]] \n",
      "\n",
      "Loss for iteration 6: 1.373271\n",
      "Iteration 0\n",
      "[[ 0.7569859   0.24301414]\n",
      " [ 0.7568801   0.24311994]\n",
      " [ 0.7569859   0.24301414]\n",
      " [ 0.75609791  0.24390204]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75675273  0.24324727]\n",
      " [ 0.75619781  0.24380225]\n",
      " [ 0.75657523  0.2434248 ]\n",
      " [ 0.75675273  0.24324727]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75620073  0.24379927]\n",
      " [ 0.75428593  0.24571401]\n",
      " [ 0.75688112  0.24311888]\n",
      " [ 0.74817467  0.25182533]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75236839  0.24763159]\n",
      " [ 0.75356561  0.24643436]\n",
      " [ 0.75152737  0.24847262]\n",
      " [ 0.75376338  0.24623658]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74942636  0.25057366]\n",
      " [ 0.74693596  0.25306401]\n",
      " [ 0.7527864   0.24721363]\n",
      " [ 0.749672    0.25032791]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.750211    0.24978898]\n",
      " [ 0.74317706  0.25682291]\n",
      " [ 0.74816829  0.25183168]\n",
      " [ 0.74710429  0.25289574]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74260151  0.25739846]\n",
      " [ 0.74068677  0.25931329]\n",
      " [ 0.74731028  0.25268975]\n",
      " [ 0.74189276  0.2581073 ]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.73491925  0.26508078]\n",
      " [ 0.73361033  0.26638973]\n",
      " [ 0.73788083  0.26211914]\n",
      " [ 0.7336511   0.26634884]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73466331  0.26533672]\n",
      " [ 0.73994428  0.26005563]\n",
      " [ 0.73880213  0.26119784]\n",
      " [ 0.73802167  0.26197839]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72638732  0.27361265]\n",
      " [ 0.73247969  0.26752025]\n",
      " [ 0.73514247  0.26485747]\n",
      " [ 0.73615611  0.26384386]] \n",
      "\n",
      "Loss for iteration 7: 1.379188\n",
      "Iteration 0\n",
      "[[ 0.75540835  0.24459165]\n",
      " [ 0.75683099  0.24316901]\n",
      " [ 0.75783592  0.24216406]\n",
      " [ 0.75600606  0.24399392]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75480556  0.24519442]\n",
      " [ 0.75723422  0.2427658 ]\n",
      " [ 0.75740302  0.24259697]\n",
      " [ 0.75480556  0.24519442]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75413954  0.24586041]\n",
      " [ 0.7559166   0.24408342]\n",
      " [ 0.75517994  0.24482009]\n",
      " [ 0.75435913  0.24564084]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7494719   0.25052816]\n",
      " [ 0.75267673  0.24732324]\n",
      " [ 0.75244176  0.24755827]\n",
      " [ 0.75393474  0.24606532]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.748887    0.25111303]\n",
      " [ 0.75253487  0.24746512]\n",
      " [ 0.74932003  0.25067991]\n",
      " [ 0.74983436  0.25016558]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.74316508  0.25683489]\n",
      " [ 0.7423526   0.25764745]\n",
      " [ 0.74587071  0.25412932]\n",
      " [ 0.74785715  0.25214288]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74429315  0.25570685]\n",
      " [ 0.74106759  0.25893238]\n",
      " [ 0.74457079  0.25542921]\n",
      " [ 0.73414373  0.26585627]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74324608  0.25675389]\n",
      " [ 0.73696232  0.26303759]\n",
      " [ 0.74650365  0.25349635]\n",
      " [ 0.73600537  0.26399463]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.7415756   0.2584244 ]\n",
      " [ 0.73321557  0.2667844 ]\n",
      " [ 0.73506695  0.26493311]\n",
      " [ 0.73730153  0.2626985 ]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72702855  0.27297148]\n",
      " [ 0.73308104  0.26691902]\n",
      " [ 0.74636948  0.25363055]\n",
      " [ 0.73738444  0.26261556]] \n",
      "\n",
      "Loss for iteration 8: 1.385032\n",
      "Iteration 0\n",
      "[[ 0.75676662  0.24323341]\n",
      " [ 0.7578935   0.2421065 ]\n",
      " [ 0.75676662  0.24323341]\n",
      " [ 0.75697774  0.24302222]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75392294  0.24607709]\n",
      " [ 0.75263494  0.24736504]\n",
      " [ 0.75392294  0.24607709]\n",
      " [ 0.75738937  0.24261068]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75679821  0.24320181]\n",
      " [ 0.7549125   0.2450875 ]\n",
      " [ 0.74927425  0.25072578]\n",
      " [ 0.74751675  0.25248331]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75561267  0.24438727]\n",
      " [ 0.75450838  0.24549165]\n",
      " [ 0.75034809  0.24965194]\n",
      " [ 0.75473893  0.24526101]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.75073808  0.24926189]\n",
      " [ 0.75064552  0.24935444]\n",
      " [ 0.75414199  0.24585803]\n",
      " [ 0.74972683  0.25027314]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.73789048  0.26210958]\n",
      " [ 0.74052298  0.25947702]\n",
      " [ 0.74168742  0.25831261]\n",
      " [ 0.7463938   0.25360617]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74978203  0.25021791]\n",
      " [ 0.74260259  0.25739744]\n",
      " [ 0.7401579   0.25984201]\n",
      " [ 0.74837708  0.25162295]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.73666149  0.26333851]\n",
      " [ 0.73680264  0.26319733]\n",
      " [ 0.74700361  0.25299636]\n",
      " [ 0.74327368  0.25672635]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73130357  0.26869643]\n",
      " [ 0.7318328   0.2681672 ]\n",
      " [ 0.72686476  0.2731353 ]\n",
      " [ 0.73030096  0.26969904]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72770309  0.27229697]\n",
      " [ 0.74410844  0.25589156]\n",
      " [ 0.72931033  0.27068964]\n",
      " [ 0.73326802  0.26673198]] \n",
      "\n",
      "Loss for iteration 9: 1.359933\n",
      "Iteration 0\n",
      "[[ 0.75526506  0.24473494]\n",
      " [ 0.75668168  0.2433183 ]\n",
      " [ 0.75668168  0.2433183 ]\n",
      " [ 0.75526506  0.24473494]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75458485  0.24541518]\n",
      " [ 0.75620162  0.24379838]\n",
      " [ 0.75457883  0.24542119]\n",
      " [ 0.75713307  0.24286695]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75228339  0.24771667]\n",
      " [ 0.74843693  0.25156316]\n",
      " [ 0.75617814  0.24382187]\n",
      " [ 0.7555449   0.24445508]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7527734   0.24722657]\n",
      " [ 0.75574672  0.24425325]\n",
      " [ 0.74997169  0.25002831]\n",
      " [ 0.74926287  0.25073713]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74590915  0.25409085]\n",
      " [ 0.75066185  0.24933816]\n",
      " [ 0.75527596  0.24472398]\n",
      " [ 0.74212164  0.25787842]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.7375387   0.2624613 ]\n",
      " [ 0.74037814  0.25962183]\n",
      " [ 0.74609041  0.25390956]\n",
      " [ 0.75041687  0.24958308]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.73759532  0.26240468]\n",
      " [ 0.73908311  0.26091689]\n",
      " [ 0.73158687  0.2684131 ]\n",
      " [ 0.73151195  0.26848808]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74153948  0.25846052]\n",
      " [ 0.7303912   0.26960877]\n",
      " [ 0.73142958  0.26857036]\n",
      " [ 0.73745757  0.26254246]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73354614  0.26645386]\n",
      " [ 0.73446226  0.26553771]\n",
      " [ 0.7262916   0.27370834]\n",
      " [ 0.73225415  0.26774588]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73788041  0.26211965]\n",
      " [ 0.73029816  0.26970178]\n",
      " [ 0.74422944  0.25577056]\n",
      " [ 0.72518122  0.27481872]] \n",
      "\n",
      "Loss for iteration 10: 1.360134\n",
      "Iteration 0\n",
      "[[ 0.75657493  0.24342509]\n",
      " [ 0.75802803  0.24197201]\n",
      " [ 0.75696748  0.2430325 ]\n",
      " [ 0.75657493  0.24342509]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75790709  0.24209292]\n",
      " [ 0.7553004   0.2446996 ]\n",
      " [ 0.75790709  0.24209292]\n",
      " [ 0.75302315  0.24697684]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75130111  0.24869893]\n",
      " [ 0.75539851  0.24460149]\n",
      " [ 0.75337857  0.2466215 ]\n",
      " [ 0.75424051  0.2457595 ]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74640256  0.25359741]\n",
      " [ 0.75215346  0.24784654]\n",
      " [ 0.74584299  0.25415698]\n",
      " [ 0.75184458  0.24815546]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74315906  0.25684091]\n",
      " [ 0.74028057  0.2597194 ]\n",
      " [ 0.73661798  0.26338202]\n",
      " [ 0.74605083  0.2539492 ]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.74538785  0.25461215]\n",
      " [ 0.74299997  0.25700003]\n",
      " [ 0.74388802  0.25611195]\n",
      " [ 0.7414993   0.25850075]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.72812963  0.27187043]\n",
      " [ 0.74951196  0.25048804]\n",
      " [ 0.72974586  0.27025416]\n",
      " [ 0.73332006  0.26667994]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.73385394  0.26614606]\n",
      " [ 0.74086559  0.25913438]\n",
      " [ 0.73026007  0.26973993]\n",
      " [ 0.74198121  0.25801876]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73318481  0.26681522]\n",
      " [ 0.74286038  0.25713959]\n",
      " [ 0.73473966  0.26526037]\n",
      " [ 0.71907723  0.28092277]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73121309  0.26878697]\n",
      " [ 0.72856098  0.27143905]\n",
      " [ 0.71956748  0.28043252]\n",
      " [ 0.73042154  0.26957849]] \n",
      "\n",
      "Loss for iteration 11: 1.363937\n",
      "Iteration 0\n",
      "[[ 0.75643742  0.24356259]\n",
      " [ 0.75643742  0.24356259]\n",
      " [ 0.75696123  0.24303877]\n",
      " [ 0.75810659  0.24189341]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75543284  0.24456714]\n",
      " [ 0.75525737  0.24474262]\n",
      " [ 0.75468785  0.2453122 ]\n",
      " [ 0.75617677  0.24382314]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75213069  0.24786936]\n",
      " [ 0.7533105   0.24668948]\n",
      " [ 0.75464553  0.24535444]\n",
      " [ 0.75331175  0.24668819]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74520057  0.2547994 ]\n",
      " [ 0.74490571  0.25509435]\n",
      " [ 0.74543327  0.25456679]\n",
      " [ 0.7471329   0.25286707]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74472839  0.25527161]\n",
      " [ 0.74821836  0.25178167]\n",
      " [ 0.74808693  0.25191307]\n",
      " [ 0.75182599  0.24817406]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.74052495  0.25947505]\n",
      " [ 0.74502254  0.25497746]\n",
      " [ 0.74759233  0.25240761]\n",
      " [ 0.73774099  0.26225901]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.72378772  0.27621225]\n",
      " [ 0.73083931  0.26916066]\n",
      " [ 0.73617023  0.26382986]\n",
      " [ 0.73602891  0.26397112]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.72555619  0.27444384]\n",
      " [ 0.74453676  0.25546321]\n",
      " [ 0.72731888  0.27268109]\n",
      " [ 0.74134487  0.25865516]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73312235  0.26687765]\n",
      " [ 0.73766017  0.2623398 ]\n",
      " [ 0.75126213  0.24873787]\n",
      " [ 0.72632492  0.27367511]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72776461  0.27223536]\n",
      " [ 0.73659605  0.26340389]\n",
      " [ 0.7161966   0.28380337]\n",
      " [ 0.73436594  0.265634  ]] \n",
      "\n",
      "Loss for iteration 12: 1.366514\n",
      "Iteration 0\n",
      "[[ 0.75536722  0.24463277]\n",
      " [ 0.75695372  0.24304624]\n",
      " [ 0.75818956  0.2418104 ]\n",
      " [ 0.75695372  0.24304624]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75165772  0.24834222]\n",
      " [ 0.7542811   0.24571891]\n",
      " [ 0.75378704  0.24621299]\n",
      " [ 0.75589067  0.24410933]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75370991  0.24629009]\n",
      " [ 0.75420159  0.24579841]\n",
      " [ 0.75220835  0.24779166]\n",
      " [ 0.75420159  0.24579841]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.75325477  0.24674527]\n",
      " [ 0.74656373  0.2534363 ]\n",
      " [ 0.74341863  0.25658134]\n",
      " [ 0.74452281  0.25547716]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.74658471  0.25341529]\n",
      " [ 0.74279219  0.25720772]\n",
      " [ 0.74800456  0.25199538]\n",
      " [ 0.73286188  0.26713812]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.72942847  0.27057162]\n",
      " [ 0.74146092  0.25853905]\n",
      " [ 0.74249309  0.257507  ]\n",
      " [ 0.73341471  0.26658532]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.73340321  0.26659676]\n",
      " [ 0.73250479  0.26749524]\n",
      " [ 0.72747475  0.27252525]\n",
      " [ 0.73026329  0.26973674]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.72918975  0.27081022]\n",
      " [ 0.74299985  0.25700021]\n",
      " [ 0.7422483   0.25775176]\n",
      " [ 0.73261851  0.26738149]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74268746  0.25731254]\n",
      " [ 0.73307216  0.26692781]\n",
      " [ 0.73767519  0.26232481]\n",
      " [ 0.74061638  0.25938359]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.74464405  0.25535595]\n",
      " [ 0.73362648  0.26637354]\n",
      " [ 0.74859232  0.25140768]\n",
      " [ 0.7417739   0.2582261 ]] \n",
      "\n",
      "Loss for iteration 13: 1.352391\n",
      "Iteration 0\n",
      "[[ 0.75604177  0.24395818]\n",
      " [ 0.75828087  0.24171908]\n",
      " [ 0.7549991   0.2450009 ]\n",
      " [ 0.7549991   0.2450009 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75620216  0.24379788]\n",
      " [ 0.7507934   0.24920656]\n",
      " [ 0.75952137  0.24047861]\n",
      " [ 0.75346571  0.24653433]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7493481   0.2506519 ]\n",
      " [ 0.74935794  0.25064209]\n",
      " [ 0.75122136  0.24877869]\n",
      " [ 0.74772114  0.25227886]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74250543  0.25749457]\n",
      " [ 0.75313151  0.24686842]\n",
      " [ 0.74321812  0.25678191]\n",
      " [ 0.74720323  0.25279677]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73900342  0.26099661]\n",
      " [ 0.73526007  0.26473993]\n",
      " [ 0.73697239  0.26302764]\n",
      " [ 0.73355973  0.26644033]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.73538399  0.26461604]\n",
      " [ 0.73088968  0.26911026]\n",
      " [ 0.73916739  0.26083267]\n",
      " [ 0.74419868  0.25580135]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.71104437  0.2889556 ]\n",
      " [ 0.72963107  0.27036887]\n",
      " [ 0.73328537  0.2667146 ]\n",
      " [ 0.73340213  0.2665979 ]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.74071687  0.25928319]\n",
      " [ 0.73596364  0.26403633]\n",
      " [ 0.74806005  0.25193992]\n",
      " [ 0.73728764  0.26271242]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73107553  0.26892442]\n",
      " [ 0.73736554  0.26263449]\n",
      " [ 0.73031265  0.26968732]\n",
      " [ 0.71288627  0.28711382]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.74096411  0.25903589]\n",
      " [ 0.7380802   0.2619198 ]\n",
      " [ 0.72714454  0.27285546]\n",
      " [ 0.74358404  0.2564159 ]] \n",
      "\n",
      "Loss for iteration 14: 1.353956\n",
      "Iteration 0\n",
      "[[ 0.75838149  0.24161847]\n",
      " [ 0.75577199  0.24422802]\n",
      " [ 0.75838149  0.24161847]\n",
      " [ 0.75693285  0.24306713]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75846308  0.24153684]\n",
      " [ 0.74984491  0.25015506]\n",
      " [ 0.74984491  0.25015506]\n",
      " [ 0.75322974  0.24677023]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.75080055  0.24919945]\n",
      " [ 0.74637181  0.25362816]\n",
      " [ 0.74736977  0.25263023]\n",
      " [ 0.74656892  0.25343114]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74459243  0.25540763]\n",
      " [ 0.73785299  0.26214698]\n",
      " [ 0.74780649  0.25219354]\n",
      " [ 0.73785299  0.26214698]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73184139  0.26815861]\n",
      " [ 0.73206192  0.26793811]\n",
      " [ 0.73206192  0.26793811]\n",
      " [ 0.73261392  0.26738605]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.73748404  0.26251596]\n",
      " [ 0.73562562  0.26437443]\n",
      " [ 0.73393917  0.26606077]\n",
      " [ 0.73052579  0.26947421]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.74358499  0.2564151 ]\n",
      " [ 0.72258699  0.27741298]\n",
      " [ 0.72348326  0.2765168 ]\n",
      " [ 0.71906918  0.28093085]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.7379458   0.26205412]\n",
      " [ 0.72763658  0.27236342]\n",
      " [ 0.72462034  0.2753796 ]\n",
      " [ 0.71305674  0.2869432 ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.74449474  0.25550526]\n",
      " [ 0.72770965  0.27229038]\n",
      " [ 0.71810949  0.28189051]\n",
      " [ 0.71253079  0.28746921]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.73768705  0.26231301]\n",
      " [ 0.72685552  0.27314448]\n",
      " [ 0.74008381  0.25991622]\n",
      " [ 0.73290014  0.26709986]] \n",
      "\n",
      "Loss for iteration 15: 1.354190\n",
      "Iteration 0\n",
      "[[ 0.75692064  0.24307929]\n",
      " [ 0.75692064  0.24307929]\n",
      " [ 0.75848526  0.24151477]\n",
      " [ 0.75692064  0.24307929]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.75281936  0.24718064]\n",
      " [ 0.75998753  0.24001247]\n",
      " [ 0.75063509  0.24936496]\n",
      " [ 0.74882275  0.25117725]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.74032879  0.25967121]\n",
      " [ 0.75318009  0.24681988]\n",
      " [ 0.7447114   0.25528857]\n",
      " [ 0.74711317  0.25288686]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.73228663  0.26771337]\n",
      " [ 0.74253917  0.25746083]\n",
      " [ 0.73525894  0.26474109]\n",
      " [ 0.73997653  0.26002347]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73396301  0.26603696]\n",
      " [ 0.74175853  0.25824147]\n",
      " [ 0.72777861  0.27222133]\n",
      " [ 0.7369101   0.2630899 ]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.72700506  0.27299494]\n",
      " [ 0.72631365  0.27368638]\n",
      " [ 0.71384603  0.28615397]\n",
      " [ 0.71597308  0.28402689]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.73964328  0.26035669]\n",
      " [ 0.72812408  0.27187598]\n",
      " [ 0.72120064  0.27879936]\n",
      " [ 0.73255289  0.26744711]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.73501551  0.26498446]\n",
      " [ 0.72494358  0.27505636]\n",
      " [ 0.7333706   0.2666294 ]\n",
      " [ 0.73209012  0.26790988]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.72662711  0.27337283]\n",
      " [ 0.72568965  0.27431038]\n",
      " [ 0.72537339  0.27462661]\n",
      " [ 0.71193355  0.28806645]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.74082863  0.25917134]\n",
      " [ 0.71755368  0.2824463 ]\n",
      " [ 0.71304148  0.28695852]\n",
      " [ 0.73569405  0.26430592]] \n",
      "\n",
      "Loss for iteration 16: 1.327101\n",
      "Iteration 0\n",
      "[[ 0.75481772  0.24518222]\n",
      " [ 0.75481772  0.24518222]\n",
      " [ 0.75502521  0.24497478]\n",
      " [ 0.75502521  0.24497478]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.758757    0.24124297]\n",
      " [ 0.74926543  0.25073451]\n",
      " [ 0.75604886  0.2439511 ]\n",
      " [ 0.758757    0.24124297]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.74265462  0.25734532]\n",
      " [ 0.75425571  0.24574427]\n",
      " [ 0.7437278   0.2562722 ]\n",
      " [ 0.74265462  0.25734532]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.74292499  0.25707507]\n",
      " [ 0.73884058  0.26115942]\n",
      " [ 0.73336953  0.26663041]\n",
      " [ 0.73884058  0.26115942]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73440492  0.26559505]\n",
      " [ 0.72992009  0.27007991]\n",
      " [ 0.72330457  0.27669543]\n",
      " [ 0.72277248  0.27722749]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.73176849  0.26823157]\n",
      " [ 0.72038084  0.27961916]\n",
      " [ 0.7289474   0.2710526 ]\n",
      " [ 0.72233522  0.27766484]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.7290262   0.27097374]\n",
      " [ 0.73579776  0.26420224]\n",
      " [ 0.7290262   0.27097374]\n",
      " [ 0.71845418  0.28154582]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71848834  0.28151163]\n",
      " [ 0.7025336   0.29746643]\n",
      " [ 0.73589903  0.26410094]\n",
      " [ 0.7285254   0.2714746 ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73621041  0.26378962]\n",
      " [ 0.73069406  0.26930594]\n",
      " [ 0.73621041  0.26378962]\n",
      " [ 0.70085484  0.29914516]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.71444041  0.28555959]\n",
      " [ 0.73664171  0.26335832]\n",
      " [ 0.71446866  0.28553134]\n",
      " [ 0.73664171  0.26335832]] \n",
      "\n",
      "Loss for iteration 17: 1.324306\n",
      "Iteration 0\n",
      "[[ 0.75688863  0.24311137]\n",
      " [ 0.75872171  0.24127831]\n",
      " [ 0.75688863  0.24311137]\n",
      " [ 0.75476295  0.2452371 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74963385  0.25036618]\n",
      " [ 0.75545132  0.24454863]\n",
      " [ 0.74675471  0.25324523]\n",
      " [ 0.74723524  0.25276479]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.74023557  0.2597644 ]\n",
      " [ 0.73414785  0.26585215]\n",
      " [ 0.7368052   0.26319483]\n",
      " [ 0.75704336  0.24295671]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.73594791  0.26405206]\n",
      " [ 0.73594791  0.26405206]\n",
      " [ 0.73000658  0.26999336]\n",
      " [ 0.7336272   0.2663728 ]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73366815  0.26633185]\n",
      " [ 0.72612917  0.27387083]\n",
      " [ 0.73366815  0.26633185]\n",
      " [ 0.72278637  0.27721357]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.725905    0.274095  ]\n",
      " [ 0.73256606  0.26743397]\n",
      " [ 0.72449738  0.27550265]\n",
      " [ 0.71450007  0.28549993]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69291502  0.30708501]\n",
      " [ 0.7131055   0.28689447]\n",
      " [ 0.71286082  0.28713918]\n",
      " [ 0.7321502   0.26784977]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.73213071  0.26786926]\n",
      " [ 0.71220303  0.28779694]\n",
      " [ 0.70610189  0.29389805]\n",
      " [ 0.72822291  0.27177709]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.73233199  0.26766801]\n",
      " [ 0.71211672  0.28788331]\n",
      " [ 0.73233199  0.26766801]\n",
      " [ 0.72586536  0.27413464]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72825074  0.27174923]\n",
      " [ 0.73181981  0.26818025]\n",
      " [ 0.67872477  0.32127517]\n",
      " [ 0.68978536  0.31021461]] \n",
      "\n",
      "Loss for iteration 18: 1.329242\n",
      "Iteration 0\n",
      "[[ 0.75395405  0.24604596]\n",
      " [ 0.75425905  0.24574097]\n",
      " [ 0.75395405  0.24604596]\n",
      " [ 0.75687021  0.24312983]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74831647  0.25168353]\n",
      " [ 0.74580193  0.25419804]\n",
      " [ 0.74552131  0.25447872]\n",
      " [ 0.74630213  0.2536979 ]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.74301463  0.25698531]\n",
      " [ 0.73785007  0.2621499 ]\n",
      " [ 0.73785007  0.2621499 ]\n",
      " [ 0.73114139  0.26885861]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.72829449  0.27170548]\n",
      " [ 0.73573488  0.26426506]\n",
      " [ 0.72683901  0.27316099]\n",
      " [ 0.71992153  0.28007847]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.73171377  0.26828623]\n",
      " [ 0.71314561  0.28685439]\n",
      " [ 0.7305972   0.2694028 ]\n",
      " [ 0.72650605  0.27349398]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70613217  0.29386786]\n",
      " [ 0.72930288  0.27069709]\n",
      " [ 0.72381133  0.27618867]\n",
      " [ 0.72072208  0.27927795]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.7287513   0.27124867]\n",
      " [ 0.7287513   0.27124867]\n",
      " [ 0.7287513   0.27124867]\n",
      " [ 0.7287513   0.27124867]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.72327918  0.27672079]\n",
      " [ 0.72861874  0.27138129]\n",
      " [ 0.72327918  0.27672079]\n",
      " [ 0.72076094  0.27923903]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.72870928  0.27129081]\n",
      " [ 0.72151941  0.27848065]\n",
      " [ 0.72151941  0.27848065]\n",
      " [ 0.70607579  0.29392421]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72499305  0.27500695]\n",
      " [ 0.72890753  0.27109247]\n",
      " [ 0.71864593  0.28135407]\n",
      " [ 0.7061401   0.29385996]] \n",
      "\n",
      "Loss for iteration 19: 1.298918\n",
      "Iteration 0\n",
      "[[ 0.75467026  0.24532981]\n",
      " [ 0.75898057  0.2410194 ]\n",
      " [ 0.75467026  0.24532981]\n",
      " [ 0.7532919   0.2467081 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74706405  0.25293601]\n",
      " [ 0.74484962  0.25515041]\n",
      " [ 0.74378198  0.25621805]\n",
      " [ 0.74706405  0.25293601]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72831339  0.27168658]\n",
      " [ 0.73554873  0.26445127]\n",
      " [ 0.73309171  0.26690829]\n",
      " [ 0.73554873  0.26445127]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.73052835  0.26947168]\n",
      " [ 0.7279287   0.2720713 ]\n",
      " [ 0.72686911  0.27313089]\n",
      " [ 0.72648519  0.27351484]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.72773004  0.27226996]\n",
      " [ 0.72259575  0.27740425]\n",
      " [ 0.70882261  0.29117739]\n",
      " [ 0.72259575  0.27740425]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.72627813  0.27372187]\n",
      " [ 0.72084099  0.27915907]\n",
      " [ 0.72844577  0.27155423]\n",
      " [ 0.70455903  0.295441  ]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.72036064  0.27963942]\n",
      " [ 0.72560751  0.27439243]\n",
      " [ 0.72036064  0.27963942]\n",
      " [ 0.72560751  0.27439243]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71602744  0.28397253]\n",
      " [ 0.72061527  0.27938476]\n",
      " [ 0.71519238  0.28480762]\n",
      " [ 0.72536874  0.27463132]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.72060651  0.27939346]\n",
      " [ 0.72126615  0.27873385]\n",
      " [ 0.72535342  0.27464661]\n",
      " [ 0.69744211  0.30255792]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.72544295  0.27455705]\n",
      " [ 0.71185356  0.28814641]\n",
      " [ 0.72544295  0.27455705]\n",
      " [ 0.72544295  0.27455705]] \n",
      "\n",
      "Loss for iteration 20: 1.300150\n",
      "Iteration 0\n",
      "[[ 0.75381887  0.24618118]\n",
      " [ 0.75381887  0.24618118]\n",
      " [ 0.75256199  0.24743809]\n",
      " [ 0.75256199  0.24743809]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74594623  0.25405371]\n",
      " [ 0.74207151  0.25792849]\n",
      " [ 0.75523031  0.24476971]\n",
      " [ 0.74207151  0.25792849]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.73338443  0.2666156 ]\n",
      " [ 0.73257399  0.26742601]\n",
      " [ 0.73338443  0.2666156 ]\n",
      " [ 0.72579062  0.27420944]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.73092431  0.26907563]\n",
      " [ 0.73376685  0.26623318]\n",
      " [ 0.72809011  0.27190986]\n",
      " [ 0.72154367  0.27845633]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.72081798  0.27918202]\n",
      " [ 0.72510445  0.27489558]\n",
      " [ 0.70499277  0.29500723]\n",
      " [ 0.71641773  0.28358236]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71885747  0.28114253]\n",
      " [ 0.7235232   0.27647686]\n",
      " [ 0.7235232   0.27647686]\n",
      " [ 0.70364583  0.29635417]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.72275001  0.27725005]\n",
      " [ 0.69772989  0.30227011]\n",
      " [ 0.69465977  0.3053402 ]\n",
      " [ 0.71339524  0.28660476]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.72241682  0.27758321]\n",
      " [ 0.72241682  0.27758321]\n",
      " [ 0.71353817  0.28646183]\n",
      " [ 0.72241682  0.27758321]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.71886164  0.28113839]\n",
      " [ 0.72230852  0.27769151]\n",
      " [ 0.72230852  0.27769151]\n",
      " [ 0.72230852  0.27769151]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.71389633  0.28610367]\n",
      " [ 0.69548005  0.30451992]\n",
      " [ 0.69494128  0.30505872]\n",
      " [ 0.72230607  0.2776939 ]] \n",
      "\n",
      "Loss for iteration 21: 1.293724\n",
      "Iteration 0\n",
      "[[ 0.75175709  0.24824288]\n",
      " [ 0.75175709  0.24824288]\n",
      " [ 0.75460809  0.24539185]\n",
      " [ 0.75460809  0.24539185]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74499083  0.25500917]\n",
      " [ 0.74036539  0.25963455]\n",
      " [ 0.74499083  0.25500917]\n",
      " [ 0.74036539  0.25963455]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.73161948  0.26838052]\n",
      " [ 0.7235651   0.27643487]\n",
      " [ 0.73130083  0.26869914]\n",
      " [ 0.73161948  0.26838052]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71165419  0.28834584]\n",
      " [ 0.71003509  0.28996494]\n",
      " [ 0.72577238  0.27422759]\n",
      " [ 0.72577238  0.27422759]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70161432  0.29838571]\n",
      " [ 0.70161432  0.29838571]\n",
      " [ 0.72263044  0.27736959]\n",
      " [ 0.71936029  0.28063968]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.72094059  0.27905938]\n",
      " [ 0.72094059  0.27905938]\n",
      " [ 0.72167552  0.27832451]\n",
      " [ 0.72094059  0.27905938]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.72007984  0.27992016]\n",
      " [ 0.72007984  0.27992016]\n",
      " [ 0.72007984  0.27992016]\n",
      " [ 0.69378608  0.30621392]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71966648  0.28033352]\n",
      " [ 0.69612533  0.3038747 ]\n",
      " [ 0.71966648  0.28033352]\n",
      " [ 0.71641457  0.2835854 ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.71948165  0.28051832]\n",
      " [ 0.71948165  0.28051832]\n",
      " [ 0.71948165  0.28051832]\n",
      " [ 0.7122215   0.28777853]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.7194072   0.28059283]\n",
      " [ 0.7194072   0.28059283]\n",
      " [ 0.71177191  0.28822806]\n",
      " [ 0.7194072   0.28059283]] \n",
      "\n",
      "Loss for iteration 22: 1.298356\n",
      "Iteration 0\n",
      "[[ 0.75090271  0.2490973 ]\n",
      " [ 0.75090271  0.2490973 ]\n",
      " [ 0.75090271  0.2490973 ]\n",
      " [ 0.75090271  0.2490973 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74128479  0.25871518]\n",
      " [ 0.75933635  0.24066365]\n",
      " [ 0.7387014   0.26129863]\n",
      " [ 0.7387014   0.26129863]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.73085821  0.26914176]\n",
      " [ 0.72829187  0.2717081 ]\n",
      " [ 0.72931731  0.27068266]\n",
      " [ 0.72169     0.27830994]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.72358441  0.27641565]\n",
      " [ 0.72358441  0.27641565]\n",
      " [ 0.72358441  0.27641565]\n",
      " [ 0.70758098  0.29241902]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71212006  0.28788   ]\n",
      " [ 0.72030991  0.27969015]\n",
      " [ 0.72030991  0.27969015]\n",
      " [ 0.72444367  0.2755563 ]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71852851  0.28147143]\n",
      " [ 0.71852851  0.28147143]\n",
      " [ 0.71852851  0.28147143]\n",
      " [ 0.71852851  0.28147143]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.690458    0.30954203]\n",
      " [ 0.70836103  0.29163903]\n",
      " [ 0.690458    0.30954203]\n",
      " [ 0.71759468  0.28240529]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71711648  0.2828835 ]\n",
      " [ 0.71711648  0.2828835 ]\n",
      " [ 0.71711648  0.2828835 ]\n",
      " [ 0.71481323  0.28518674]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.71511817  0.28488183]\n",
      " [ 0.68965667  0.3103433 ]\n",
      " [ 0.71687275  0.28312722]\n",
      " [ 0.71687275  0.28312722]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.71674615  0.28325382]\n",
      " [ 0.71674615  0.28325382]\n",
      " [ 0.71674615  0.28325382]\n",
      " [ 0.71674615  0.28325382]] \n",
      "\n",
      "Loss for iteration 23: 1.247231\n",
      "Iteration 0\n",
      "[[ 0.75003123  0.2499688 ]\n",
      " [ 0.75003123  0.2499688 ]\n",
      " [ 0.75457066  0.24542935]\n",
      " [ 0.75457066  0.24542935]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73711103  0.262889  ]\n",
      " [ 0.73711103  0.262889  ]\n",
      " [ 0.73711103  0.262889  ]\n",
      " [ 0.73711103  0.262889  ]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72744447  0.27255553]\n",
      " [ 0.7302683   0.26973173]\n",
      " [ 0.72744447  0.27255553]\n",
      " [ 0.72744447  0.27255553]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71659297  0.28340703]\n",
      " [ 0.72328889  0.27671108]\n",
      " [ 0.72196662  0.27803338]\n",
      " [ 0.72152627  0.27847373]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71813446  0.28186551]\n",
      " [ 0.71813446  0.28186551]\n",
      " [ 0.69625765  0.30374238]\n",
      " [ 0.71813446  0.28186551]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71627355  0.28372651]\n",
      " [ 0.70768893  0.29231101]\n",
      " [ 0.69075751  0.30924249]\n",
      " [ 0.71627355  0.28372651]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.71375     0.28625   ]\n",
      " [ 0.71527779  0.28472221]\n",
      " [ 0.71527779  0.28472221]\n",
      " [ 0.71527779  0.28472221]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71474749  0.28525257]\n",
      " [ 0.68574917  0.31425086]\n",
      " [ 0.71474749  0.28525257]\n",
      " [ 0.68574917  0.31425086]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.71445942  0.28554064]\n",
      " [ 0.71211022  0.28788975]\n",
      " [ 0.70662332  0.29337674]\n",
      " [ 0.71445942  0.28554064]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.71415055  0.28584945]\n",
      " [ 0.71429628  0.28570369]\n",
      " [ 0.71415055  0.28584945]\n",
      " [ 0.68435258  0.31564742]] \n",
      "\n",
      "Loss for iteration 24: 1.285135\n",
      "Iteration 0\n",
      "[[ 0.74915338  0.25084665]\n",
      " [ 0.74915338  0.25084665]\n",
      " [ 0.74915338  0.25084665]\n",
      " [ 0.74915338  0.25084665]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.74321306  0.25678694]\n",
      " [ 0.73559445  0.26440558]\n",
      " [ 0.73559445  0.26440558]\n",
      " [ 0.73559445  0.26440558]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72566885  0.27433109]\n",
      " [ 0.72979027  0.27020967]\n",
      " [ 0.72566885  0.27433109]\n",
      " [ 0.72566885  0.27433109]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71957958  0.28042042]\n",
      " [ 0.71957958  0.28042042]\n",
      " [ 0.71957958  0.28042042]\n",
      " [ 0.70368218  0.29631779]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71608222  0.28391778]\n",
      " [ 0.71608222  0.28391778]\n",
      " [ 0.71608222  0.28391778]\n",
      " [ 0.71608222  0.28391778]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71415138  0.28584865]\n",
      " [ 0.71415138  0.28584865]\n",
      " [ 0.68847615  0.31152388]\n",
      " [ 0.71415138  0.28584865]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.71310431  0.28689569]\n",
      " [ 0.71310431  0.28689569]\n",
      " [ 0.71310431  0.28689569]\n",
      " [ 0.71310431  0.28689569]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71253395  0.28746605]\n",
      " [ 0.71253395  0.28746605]\n",
      " [ 0.71253395  0.28746605]\n",
      " [ 0.71253395  0.28746605]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70499605  0.29500392]\n",
      " [ 0.71221489  0.28778508]\n",
      " [ 0.71221489  0.28778508]\n",
      " [ 0.71221489  0.28778508]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.7120291   0.2879709 ]\n",
      " [ 0.7120291   0.2879709 ]\n",
      " [ 0.7120291   0.2879709 ]\n",
      " [ 0.71285462  0.28714538]] \n",
      "\n",
      "Loss for iteration 25: 1.277516\n",
      "Iteration 0\n",
      "[[ 0.74827772  0.25172228]\n",
      " [ 0.74827772  0.25172228]\n",
      " [ 0.7566582   0.24334179]\n",
      " [ 0.74827772  0.25172228]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73414809  0.26585197]\n",
      " [ 0.7422151   0.2577849 ]\n",
      " [ 0.7422151   0.2577849 ]\n",
      " [ 0.73414809  0.26585197]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72397649  0.27602351]\n",
      " [ 0.72397649  0.27602351]\n",
      " [ 0.72397649  0.27602351]\n",
      " [ 0.72397649  0.27602351]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71772546  0.28227451]\n",
      " [ 0.71772546  0.28227451]\n",
      " [ 0.71772546  0.28227451]\n",
      " [ 0.70220309  0.29779693]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71413004  0.28586993]\n",
      " [ 0.71579713  0.2842029 ]\n",
      " [ 0.71579713  0.2842029 ]\n",
      " [ 0.71413004  0.28586993]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71213639  0.28786355]\n",
      " [ 0.71213639  0.28786355]\n",
      " [ 0.71213639  0.28786355]\n",
      " [ 0.71213639  0.28786355]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.71104616  0.28895393]\n",
      " [ 0.71188462  0.28811529]\n",
      " [ 0.70393836  0.29606164]\n",
      " [ 0.71104616  0.28895393]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.71044534  0.28955466]\n",
      " [ 0.71044534  0.28955466]\n",
      " [ 0.70354998  0.29645002]\n",
      " [ 0.71044534  0.28955466]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.71010596  0.28989407]\n",
      " [ 0.71010596  0.28989407]\n",
      " [ 0.68009192  0.31990805]\n",
      " [ 0.71010596  0.28989407]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70990807  0.29009193]\n",
      " [ 0.70990807  0.29009193]\n",
      " [ 0.70990807  0.29009193]\n",
      " [ 0.70990807  0.29009193]] \n",
      "\n",
      "Loss for iteration 26: 1.273901\n",
      "Iteration 0\n",
      "[[ 0.74744147  0.25255853]\n",
      " [ 0.74744147  0.25255853]\n",
      " [ 0.74744147  0.25255853]\n",
      " [ 0.74744147  0.25255853]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73281449  0.26718551]\n",
      " [ 0.73281449  0.26718551]\n",
      " [ 0.73281449  0.26718551]\n",
      " [ 0.73281449  0.26718551]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72241068  0.27758935]\n",
      " [ 0.72241068  0.27758935]\n",
      " [ 0.72241068  0.27758935]\n",
      " [ 0.72241068  0.27758935]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71600783  0.2839922 ]\n",
      " [ 0.71600783  0.2839922 ]\n",
      " [ 0.71600783  0.2839922 ]\n",
      " [ 0.70097625  0.29902378]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71232098  0.28767905]\n",
      " [ 0.71232098  0.28767905]\n",
      " [ 0.71232098  0.28767905]\n",
      " [ 0.71232098  0.28767905]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71026987  0.2897301 ]\n",
      " [ 0.71026987  0.2897301 ]\n",
      " [ 0.71026987  0.2897301 ]\n",
      " [ 0.68495655  0.31504345]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70914209  0.29085791]\n",
      " [ 0.70515066  0.29484928]\n",
      " [ 0.70914209  0.29085791]\n",
      " [ 0.70914209  0.29085791]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.70851755  0.29148251]\n",
      " [ 0.70851755  0.29148251]\n",
      " [ 0.67941165  0.32058838]\n",
      " [ 0.70851755  0.29148251]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70816475  0.29183528]\n",
      " [ 0.70816475  0.29183528]\n",
      " [ 0.70816475  0.29183528]\n",
      " [ 0.70816475  0.29183528]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70796144  0.29203856]\n",
      " [ 0.70796144  0.29203856]\n",
      " [ 0.70796144  0.29203856]\n",
      " [ 0.70796144  0.29203856]] \n",
      "\n",
      "Loss for iteration 27: 1.264671\n",
      "Iteration 0\n",
      "[[ 0.74665797  0.253342  ]\n",
      " [ 0.74665797  0.253342  ]\n",
      " [ 0.74665797  0.253342  ]\n",
      " [ 0.74665797  0.253342  ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73160124  0.26839873]\n",
      " [ 0.73160124  0.26839873]\n",
      " [ 0.73160124  0.26839873]\n",
      " [ 0.73160124  0.26839873]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.72097927  0.27902073]\n",
      " [ 0.72551501  0.27448499]\n",
      " [ 0.72097927  0.27902073]\n",
      " [ 0.72097927  0.27902073]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71443468  0.28556523]\n",
      " [ 0.71443468  0.28556523]\n",
      " [ 0.71443468  0.28556523]\n",
      " [ 0.71443468  0.28556523]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.71066242  0.28933755]\n",
      " [ 0.71066242  0.28933755]\n",
      " [ 0.71066242  0.28933755]\n",
      " [ 0.71066242  0.28933755]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.71183735  0.28816265]\n",
      " [ 0.70855814  0.29144186]\n",
      " [ 0.70855814  0.29144186]\n",
      " [ 0.70855814  0.29144186]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70739698  0.29260302]\n",
      " [ 0.70739698  0.29260302]\n",
      " [ 0.68000394  0.31999609]\n",
      " [ 0.70739698  0.29260302]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.7067529   0.2932471 ]\n",
      " [ 0.7067529   0.2932471 ]\n",
      " [ 0.7067529   0.2932471 ]\n",
      " [ 0.70980865  0.29019135]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70639098  0.29360902]\n",
      " [ 0.70639098  0.29360902]\n",
      " [ 0.70639098  0.29360902]\n",
      " [ 0.70639098  0.29360902]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.67628574  0.32371426]\n",
      " [ 0.70618618  0.29381385]\n",
      " [ 0.70618618  0.29381385]\n",
      " [ 0.70618618  0.29381385]] \n",
      "\n",
      "Loss for iteration 28: 1.246476\n",
      "Iteration 0\n",
      "[[ 0.74593621  0.25406376]\n",
      " [ 0.74593621  0.25406376]\n",
      " [ 0.74593621  0.25406376]\n",
      " [ 0.74593621  0.25406376]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73051208  0.26948798]\n",
      " [ 0.73051208  0.26948798]\n",
      " [ 0.73051208  0.26948798]\n",
      " [ 0.73051208  0.26948798]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71968734  0.28031263]\n",
      " [ 0.72532701  0.27467296]\n",
      " [ 0.71968734  0.28031263]\n",
      " [ 0.71968734  0.28031263]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71301192  0.28698805]\n",
      " [ 0.71301192  0.28698805]\n",
      " [ 0.71301192  0.28698805]\n",
      " [ 0.71301192  0.28698805]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70916003  0.29083994]\n",
      " [ 0.70916003  0.29083994]\n",
      " [ 0.70916003  0.29083994]\n",
      " [ 0.70916003  0.29083994]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.7070061   0.29299393]\n",
      " [ 0.7070061   0.29299393]\n",
      " [ 0.7070061   0.29299393]\n",
      " [ 0.70330554  0.29669446]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70581442  0.29418555]\n",
      " [ 0.70581442  0.29418555]\n",
      " [ 0.70581442  0.29418555]\n",
      " [ 0.70581442  0.29418555]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.70515341  0.29484656]\n",
      " [ 0.70515341  0.29484656]\n",
      " [ 0.70515341  0.29484656]\n",
      " [ 0.70515341  0.29484656]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70478457  0.29521546]\n",
      " [ 0.70478457  0.29521546]\n",
      " [ 0.70478457  0.29521546]\n",
      " [ 0.70478457  0.29521546]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70457983  0.29542011]\n",
      " [ 0.70457983  0.29542011]\n",
      " [ 0.70457983  0.29542011]\n",
      " [ 0.70457983  0.29542011]] \n",
      "\n",
      "Loss for iteration 29: 1.254840\n",
      "Iteration 0\n",
      "[[ 0.74527645  0.25472352]\n",
      " [ 0.74527645  0.25472352]\n",
      " [ 0.74527645  0.25472352]\n",
      " [ 0.74527645  0.25472352]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.73784435  0.26215559]\n",
      " [ 0.72953957  0.27046043]\n",
      " [ 0.72953957  0.27046043]\n",
      " [ 0.72953957  0.27046043]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71494931  0.28505075]\n",
      " [ 0.71852827  0.2814717 ]\n",
      " [ 0.71852827  0.2814717 ]\n",
      " [ 0.71852827  0.2814717 ]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7117328   0.28826723]\n",
      " [ 0.7117328   0.28826723]\n",
      " [ 0.7117328   0.28826723]\n",
      " [ 0.7117328   0.28826723]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70780683  0.29219314]\n",
      " [ 0.70780683  0.29219314]\n",
      " [ 0.70649749  0.29350248]\n",
      " [ 0.70780683  0.29219314]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70560634  0.29439369]\n",
      " [ 0.70560634  0.29439369]\n",
      " [ 0.71077478  0.28922519]\n",
      " [ 0.70560634  0.29439369]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70438617  0.29561388]\n",
      " [ 0.67765945  0.32234061]\n",
      " [ 0.70438617  0.29561388]\n",
      " [ 0.70438617  0.29561388]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.7037096   0.29629043]\n",
      " [ 0.7037096   0.29629043]\n",
      " [ 0.7037096   0.29629043]\n",
      " [ 0.7037096   0.29629043]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70333463  0.29666537]\n",
      " [ 0.70333463  0.29666537]\n",
      " [ 0.70333463  0.29666537]\n",
      " [ 0.70333463  0.29666537]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70313042  0.29686961]\n",
      " [ 0.70313042  0.29686961]\n",
      " [ 0.70313042  0.29686961]\n",
      " [ 0.70313042  0.29686961]] \n",
      "\n",
      "Loss for iteration 30: 1.250470\n",
      "Iteration 0\n",
      "[[ 0.74468428  0.25531566]\n",
      " [ 0.74468428  0.25531566]\n",
      " [ 0.74468428  0.25531566]\n",
      " [ 0.74468428  0.25531566]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72868508  0.27131489]\n",
      " [ 0.72868508  0.27131489]\n",
      " [ 0.72868508  0.27131489]\n",
      " [ 0.72868508  0.27131489]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7175048   0.28249526]\n",
      " [ 0.7175048   0.28249526]\n",
      " [ 0.7175048   0.28249526]\n",
      " [ 0.7175048   0.28249526]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.71060044  0.28939956]\n",
      " [ 0.71060044  0.28939956]\n",
      " [ 0.71060044  0.28939956]\n",
      " [ 0.71060044  0.28939956]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.68693078  0.31306925]\n",
      " [ 0.70660621  0.29339379]\n",
      " [ 0.70660621  0.29339379]\n",
      " [ 0.70660621  0.29339379]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70436192  0.29563805]\n",
      " [ 0.70436192  0.29563805]\n",
      " [ 0.70248264  0.29751739]\n",
      " [ 0.70436192  0.29563805]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70311445  0.29688552]\n",
      " [ 0.70311445  0.29688552]\n",
      " [ 0.70311445  0.29688552]\n",
      " [ 0.70311445  0.29688552]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.70784909  0.29215088]\n",
      " [ 0.70242262  0.29757729]\n",
      " [ 0.70242262  0.29757729]\n",
      " [ 0.70242262  0.29757729]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70204133  0.2979587 ]\n",
      " [ 0.70204133  0.2979587 ]\n",
      " [ 0.70204133  0.2979587 ]\n",
      " [ 0.70204133  0.2979587 ]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70183653  0.29816341]\n",
      " [ 0.70183653  0.29816341]\n",
      " [ 0.70183653  0.29816341]\n",
      " [ 0.70183653  0.29816341]] \n",
      "\n",
      "Loss for iteration 31: 1.246603\n",
      "Iteration 0\n",
      "[[ 0.74416089  0.25583908]\n",
      " [ 0.74416089  0.25583908]\n",
      " [ 0.74416089  0.25583908]\n",
      " [ 0.74416089  0.25583908]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72794396  0.27205604]\n",
      " [ 0.74237901  0.25762099]\n",
      " [ 0.72794396  0.27205604]\n",
      " [ 0.72794396  0.27205604]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71661252  0.28338748]\n",
      " [ 0.71661252  0.28338748]\n",
      " [ 0.71661252  0.28338748]\n",
      " [ 0.71661252  0.28338748]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70961064  0.29038936]\n",
      " [ 0.70961064  0.29038936]\n",
      " [ 0.70961064  0.29038936]\n",
      " [ 0.70961064  0.29038936]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70555377  0.29444623]\n",
      " [ 0.70555377  0.29444623]\n",
      " [ 0.70555377  0.29444623]\n",
      " [ 0.70555377  0.29444623]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70326829  0.29673174]\n",
      " [ 0.70326829  0.29673174]\n",
      " [ 0.70326829  0.29673174]\n",
      " [ 0.70326829  0.29673174]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.7019943  0.2980057]\n",
      " [ 0.7019943  0.2980057]\n",
      " [ 0.7019943  0.2980057]\n",
      " [ 0.7019943  0.2980057]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.70128697  0.29871303]\n",
      " [ 0.70128697  0.29871303]\n",
      " [ 0.70128697  0.29871303]\n",
      " [ 0.70128697  0.29871303]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.70089805  0.29910195]\n",
      " [ 0.70089805  0.29910195]\n",
      " [ 0.70089805  0.29910195]\n",
      " [ 0.70089805  0.29910195]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.70069116  0.29930881]\n",
      " [ 0.70069116  0.29930881]\n",
      " [ 0.70069116  0.29930881]\n",
      " [ 0.70069116  0.29930881]] \n",
      "\n",
      "Loss for iteration 32: 1.243218\n",
      "Iteration 0\n",
      "[[ 0.74370384  0.25629616]\n",
      " [ 0.74370384  0.25629616]\n",
      " [ 0.74370384  0.25629616]\n",
      " [ 0.74370384  0.25629616]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72730726  0.27269268]\n",
      " [ 0.72730726  0.27269268]\n",
      " [ 0.72730726  0.27269268]\n",
      " [ 0.72730726  0.27269268]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71584213  0.28415787]\n",
      " [ 0.71584213  0.28415787]\n",
      " [ 0.71584213  0.28415787]\n",
      " [ 0.71584213  0.28415787]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70875335  0.29124668]\n",
      " [ 0.70875335  0.29124668]\n",
      " [ 0.70875335  0.29124668]\n",
      " [ 0.70875335  0.29124668]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70463914  0.2953608 ]\n",
      " [ 0.70463914  0.2953608 ]\n",
      " [ 0.70463914  0.2953608 ]\n",
      " [ 0.70463914  0.2953608 ]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70231479  0.29768527]\n",
      " [ 0.70231479  0.29768527]\n",
      " [ 0.70231479  0.29768527]\n",
      " [ 0.70231479  0.29768527]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70101488  0.29898515]\n",
      " [ 0.70101488  0.29898515]\n",
      " [ 0.70101488  0.29898515]\n",
      " [ 0.70101488  0.29898515]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.70029134  0.29970863]\n",
      " [ 0.70029134  0.29970863]\n",
      " [ 0.70029134  0.29970863]\n",
      " [ 0.70029134  0.29970863]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69989347  0.30010653]\n",
      " [ 0.69989347  0.30010653]\n",
      " [ 0.69989347  0.30010653]\n",
      " [ 0.69989347  0.30010653]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69968235  0.30031767]\n",
      " [ 0.69968235  0.30031767]\n",
      " [ 0.69968235  0.30031767]\n",
      " [ 0.69968235  0.30031767]] \n",
      "\n",
      "Loss for iteration 33: 1.240276\n",
      "Iteration 0\n",
      "[[ 0.74330819  0.25669181]\n",
      " [ 0.74330819  0.25669181]\n",
      " [ 0.74330819  0.25669181]\n",
      " [ 0.74330819  0.25669181]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.7267639   0.27323607]\n",
      " [ 0.7267639   0.27323607]\n",
      " [ 0.7267639   0.27323607]\n",
      " [ 0.7267639   0.27323607]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71518111  0.28481886]\n",
      " [ 0.71518111  0.28481886]\n",
      " [ 0.71518111  0.28481886]\n",
      " [ 0.71518111  0.28481886]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7080152   0.29198483]\n",
      " [ 0.7080152   0.29198483]\n",
      " [ 0.7080152   0.29198483]\n",
      " [ 0.7080152   0.29198483]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70553517  0.29446483]\n",
      " [ 0.7038486   0.29615134]\n",
      " [ 0.7038486   0.29615134]\n",
      " [ 0.7038486   0.29615134]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70148754  0.29851252]\n",
      " [ 0.70148754  0.29851252]\n",
      " [ 0.70148754  0.29851252]\n",
      " [ 0.70148754  0.29851252]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.70016199  0.29983798]\n",
      " [ 0.70016199  0.29983798]\n",
      " [ 0.70016199  0.29983798]\n",
      " [ 0.70016199  0.29983798]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69942164  0.30057836]\n",
      " [ 0.69942164  0.30057836]\n",
      " [ 0.69942164  0.30057836]\n",
      " [ 0.69942164  0.30057836]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69901323  0.30098677]\n",
      " [ 0.69901323  0.30098677]\n",
      " [ 0.69901323  0.30098677]\n",
      " [ 0.69901323  0.30098677]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69879603  0.30120397]\n",
      " [ 0.69879603  0.30120397]\n",
      " [ 0.69879603  0.30120397]\n",
      " [ 0.69879603  0.30120397]] \n",
      "\n",
      "Loss for iteration 34: 1.237730\n",
      "Iteration 0\n",
      "[[ 0.74296772  0.25703225]\n",
      " [ 0.74296772  0.25703225]\n",
      " [ 0.74296772  0.25703225]\n",
      " [ 0.74296772  0.25703225]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72630173  0.27369824]\n",
      " [ 0.72630173  0.27369824]\n",
      " [ 0.72630173  0.27369824]\n",
      " [ 0.72630173  0.27369824]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71461576  0.28538424]\n",
      " [ 0.71461576  0.28538424]\n",
      " [ 0.71461576  0.28538424]\n",
      " [ 0.71461576  0.28538424]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70738137  0.29261866]\n",
      " [ 0.70738137  0.29261866]\n",
      " [ 0.70738137  0.29261866]\n",
      " [ 0.70738137  0.29261866]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70316696  0.29683304]\n",
      " [ 0.70316696  0.29683304]\n",
      " [ 0.70316696  0.29683304]\n",
      " [ 0.70316696  0.29683304]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70077091  0.299229  ]\n",
      " [ 0.70077091  0.299229  ]\n",
      " [ 0.70077091  0.299229  ]\n",
      " [ 0.70077091  0.299229  ]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69942033  0.3005797 ]\n",
      " [ 0.69942033  0.3005797 ]\n",
      " [ 0.69942033  0.3005797 ]\n",
      " [ 0.69942033  0.3005797 ]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.6986624   0.30133763]\n",
      " [ 0.6986624   0.30133763]\n",
      " [ 0.6986624   0.30133763]\n",
      " [ 0.6986624   0.30133763]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69824225  0.30175784]\n",
      " [ 0.69824225  0.30175784]\n",
      " [ 0.69824225  0.30175784]\n",
      " [ 0.69824225  0.30175784]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.698017    0.30198297]\n",
      " [ 0.698017    0.30198297]\n",
      " [ 0.66995466  0.33004534]\n",
      " [ 0.698017    0.30198297]] \n",
      "\n",
      "Loss for iteration 35: 1.235530\n",
      "Iteration 0\n",
      "[[ 0.74267596  0.2573241 ]\n",
      " [ 0.74267596  0.2573241 ]\n",
      " [ 0.74267596  0.2573241 ]\n",
      " [ 0.74267596  0.2573241 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72590899  0.27409098]\n",
      " [ 0.72590899  0.27409098]\n",
      " [ 0.72590899  0.27409098]\n",
      " [ 0.72590899  0.27409098]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71413249  0.28586748]\n",
      " [ 0.71413249  0.28586748]\n",
      " [ 0.71413249  0.28586748]\n",
      " [ 0.71413249  0.28586748]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7068373  0.2931627]\n",
      " [ 0.7068373  0.2931627]\n",
      " [ 0.7068373  0.2931627]\n",
      " [ 0.7068373  0.2931627]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70257902  0.29742098]\n",
      " [ 0.70257902  0.29742098]\n",
      " [ 0.70257902  0.29742098]\n",
      " [ 0.70257902  0.29742098]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.70014995  0.29985005]\n",
      " [ 0.70014995  0.29985005]\n",
      " [ 0.70014995  0.29985005]\n",
      " [ 0.70014995  0.29985005]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69877446  0.30122548]\n",
      " [ 0.69877446  0.30122548]\n",
      " [ 0.69877446  0.30122548]\n",
      " [ 0.69877446  0.30122548]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.6711328   0.32886714]\n",
      " [ 0.6979984   0.30200163]\n",
      " [ 0.6979984   0.30200163]\n",
      " [ 0.6979984   0.30200163]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.6975652   0.30243477]\n",
      " [ 0.6975652   0.30243477]\n",
      " [ 0.6975652   0.30243477]\n",
      " [ 0.6975652   0.30243477]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69733071  0.30266932]\n",
      " [ 0.69733071  0.30266932]\n",
      " [ 0.69733071  0.30266932]\n",
      " [ 0.69733071  0.30266932]] \n",
      "\n",
      "Loss for iteration 36: 1.233627\n",
      "Iteration 0\n",
      "[[ 0.74242604  0.2575739 ]\n",
      " [ 0.74242604  0.2575739 ]\n",
      " [ 0.74242604  0.2575739 ]\n",
      " [ 0.75634384  0.24365614]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72557485  0.27442515]\n",
      " [ 0.72557485  0.27442515]\n",
      " [ 0.72557485  0.27442515]\n",
      " [ 0.72557485  0.27442515]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71371877  0.28628117]\n",
      " [ 0.71371877  0.28628117]\n",
      " [ 0.71371877  0.28628117]\n",
      " [ 0.71371877  0.28628117]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70636934  0.29363063]\n",
      " [ 0.70636934  0.29363063]\n",
      " [ 0.70636934  0.29363063]\n",
      " [ 0.70636934  0.29363063]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70207071  0.2979292 ]\n",
      " [ 0.70207071  0.2979292 ]\n",
      " [ 0.70207071  0.2979292 ]\n",
      " [ 0.70207071  0.2979292 ]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69961023  0.3003898 ]\n",
      " [ 0.69961023  0.3003898 ]\n",
      " [ 0.69961023  0.3003898 ]\n",
      " [ 0.69961023  0.3003898 ]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.6982103   0.30178967]\n",
      " [ 0.6982103   0.30178967]\n",
      " [ 0.6982103   0.30178967]\n",
      " [ 0.6982103   0.30178967]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69741559  0.30258441]\n",
      " [ 0.69741559  0.30258441]\n",
      " [ 0.69741559  0.30258441]\n",
      " [ 0.69741559  0.30258441]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69696844  0.30303156]\n",
      " [ 0.69696844  0.30303156]\n",
      " [ 0.69696844  0.30303156]\n",
      " [ 0.69696844  0.30303156]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69672322  0.30327678]\n",
      " [ 0.66888922  0.33111081]\n",
      " [ 0.69672322  0.30327678]\n",
      " [ 0.69672322  0.30327678]] \n",
      "\n",
      "Loss for iteration 37: 1.231976\n",
      "Iteration 0\n",
      "[[ 0.74221218  0.25778788]\n",
      " [ 0.74221218  0.25778788]\n",
      " [ 0.74221218  0.25778788]\n",
      " [ 0.74221218  0.25778788]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.7252897   0.27471024]\n",
      " [ 0.7252897   0.27471024]\n",
      " [ 0.7252897   0.27471024]\n",
      " [ 0.7252897   0.27471024]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71336371  0.28663629]\n",
      " [ 0.71336371  0.28663629]\n",
      " [ 0.71336371  0.28663629]\n",
      " [ 0.71336371  0.28663629]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7059657   0.29403427]\n",
      " [ 0.7059657   0.29403427]\n",
      " [ 0.7059657   0.29403427]\n",
      " [ 0.7059657   0.29403427]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70162994  0.29837012]\n",
      " [ 0.70162994  0.29837012]\n",
      " [ 0.70162994  0.29837012]\n",
      " [ 0.70162994  0.29837012]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.6991393  0.3008607]\n",
      " [ 0.6991393  0.3008607]\n",
      " [ 0.6991393  0.3008607]\n",
      " [ 0.6991393  0.3008607]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69771522  0.30228475]\n",
      " [ 0.69771522  0.30228475]\n",
      " [ 0.69771522  0.30228475]\n",
      " [ 0.69771522  0.30228475]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69690162  0.30309844]\n",
      " [ 0.69690162  0.30309844]\n",
      " [ 0.69690162  0.30309844]\n",
      " [ 0.69690162  0.30309844]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69643962  0.30356038]\n",
      " [ 0.69643962  0.30356038]\n",
      " [ 0.69643962  0.30356038]\n",
      " [ 0.69643962  0.30356038]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69618261  0.30381733]\n",
      " [ 0.69618261  0.30381733]\n",
      " [ 0.69618261  0.30381733]\n",
      " [ 0.69618261  0.30381733]] \n",
      "\n",
      "Loss for iteration 38: 1.230537\n",
      "Iteration 0\n",
      "[[ 0.74202859  0.25797138]\n",
      " [ 0.74202859  0.25797138]\n",
      " [ 0.74202859  0.25797138]\n",
      " [ 0.74202859  0.25797138]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72504556  0.27495444]\n",
      " [ 0.72504556  0.27495444]\n",
      " [ 0.72504556  0.27495444]\n",
      " [ 0.72504556  0.27495444]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7130577  0.2869423]\n",
      " [ 0.7130577  0.2869423]\n",
      " [ 0.7130577  0.2869423]\n",
      " [ 0.7130577  0.2869423]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70561624  0.29438373]\n",
      " [ 0.70561624  0.29438373]\n",
      " [ 0.70561624  0.29438373]\n",
      " [ 0.70561624  0.29438373]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70124596  0.29875401]\n",
      " [ 0.70124596  0.29875401]\n",
      " [ 0.70124596  0.29875401]\n",
      " [ 0.70124596  0.29875401]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69872677  0.30127329]\n",
      " [ 0.69872677  0.30127329]\n",
      " [ 0.69872677  0.30127329]\n",
      " [ 0.69872677  0.30127329]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69727892  0.30272102]\n",
      " [ 0.69727892  0.30272102]\n",
      " [ 0.69727892  0.30272102]\n",
      " [ 0.69727892  0.30272102]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69644618  0.30355385]\n",
      " [ 0.69644618  0.30355385]\n",
      " [ 0.69644618  0.30355385]\n",
      " [ 0.69644618  0.30355385]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69596881  0.30403119]\n",
      " [ 0.66852421  0.33147576]\n",
      " [ 0.69596881  0.30403119]\n",
      " [ 0.69596881  0.30403119]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69569939  0.30430064]\n",
      " [ 0.69569939  0.30430064]\n",
      " [ 0.69569939  0.30430064]\n",
      " [ 0.69569939  0.30430064]] \n",
      "\n",
      "Loss for iteration 39: 1.229278\n",
      "Iteration 0\n",
      "[[ 0.74187082  0.25812921]\n",
      " [ 0.74187082  0.25812921]\n",
      " [ 0.74187082  0.25812921]\n",
      " [ 0.74187082  0.25812921]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72483534  0.27516463]\n",
      " [ 0.72483534  0.27516463]\n",
      " [ 0.72483534  0.27516463]\n",
      " [ 0.72483534  0.27516463]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71279281  0.28720719]\n",
      " [ 0.71279281  0.28720719]\n",
      " [ 0.71279281  0.28720719]\n",
      " [ 0.71279281  0.28720719]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70531231  0.29468766]\n",
      " [ 0.70531231  0.29468766]\n",
      " [ 0.70531231  0.29468766]\n",
      " [ 0.70531231  0.29468766]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70091021  0.29908985]\n",
      " [ 0.70091021  0.29908985]\n",
      " [ 0.70091021  0.29908985]\n",
      " [ 0.70091021  0.29908985]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.6983636   0.30163637]\n",
      " [ 0.6983636   0.30163637]\n",
      " [ 0.6983636   0.30163637]\n",
      " [ 0.6983636   0.30163637]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69689286  0.30310714]\n",
      " [ 0.69689286  0.30310714]\n",
      " [ 0.69689286  0.30310714]\n",
      " [ 0.69689286  0.30310714]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69604099  0.30395898]\n",
      " [ 0.69604099  0.30395898]\n",
      " [ 0.69604099  0.30395898]\n",
      " [ 0.69604099  0.30395898]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69554818  0.30445179]\n",
      " [ 0.69554818  0.30445179]\n",
      " [ 0.69554818  0.30445179]\n",
      " [ 0.69554818  0.30445179]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69526583  0.30473417]\n",
      " [ 0.69526583  0.30473417]\n",
      " [ 0.69526583  0.30473417]\n",
      " [ 0.69526583  0.30473417]] \n",
      "\n",
      "Loss for iteration 40: 1.228171\n",
      "Iteration 0\n",
      "[[ 0.74173462  0.25826538]\n",
      " [ 0.74173462  0.25826538]\n",
      " [ 0.74173462  0.25826538]\n",
      " [ 0.74173462  0.25826538]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72465366  0.2753464 ]\n",
      " [ 0.72465366  0.2753464 ]\n",
      " [ 0.72465366  0.2753464 ]\n",
      " [ 0.72465366  0.2753464 ]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7125625   0.28743747]\n",
      " [ 0.7125625   0.28743747]\n",
      " [ 0.7125625   0.28743747]\n",
      " [ 0.7125625   0.28743747]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70504689  0.29495305]\n",
      " [ 0.70504689  0.29495305]\n",
      " [ 0.70504689  0.29495305]\n",
      " [ 0.70504689  0.29495305]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70061541  0.29938456]\n",
      " [ 0.70061541  0.29938456]\n",
      " [ 0.70061541  0.29938456]\n",
      " [ 0.70061541  0.29938456]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69804317  0.30195689]\n",
      " [ 0.69804317  0.30195689]\n",
      " [ 0.69804317  0.30195689]\n",
      " [ 0.69804317  0.30195689]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69655013  0.30344987]\n",
      " [ 0.69655013  0.30344987]\n",
      " [ 0.69655013  0.30344987]\n",
      " [ 0.69655013  0.30344987]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69567972  0.30432025]\n",
      " [ 0.69567972  0.30432025]\n",
      " [ 0.69567972  0.30432025]\n",
      " [ 0.69567972  0.30432025]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69517148  0.30482852]\n",
      " [ 0.69517148  0.30482852]\n",
      " [ 0.69517148  0.30482852]\n",
      " [ 0.69517148  0.30482852]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69487619  0.30512375]\n",
      " [ 0.69487619  0.30512375]\n",
      " [ 0.69487619  0.30512375]\n",
      " [ 0.69487619  0.30512375]] \n",
      "\n",
      "Loss for iteration 41: 1.227194\n",
      "Iteration 0\n",
      "[[ 0.74161661  0.25838342]\n",
      " [ 0.74161661  0.25838342]\n",
      " [ 0.74161661  0.25838342]\n",
      " [ 0.74161661  0.25838342]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72449559  0.27550438]\n",
      " [ 0.72449559  0.27550438]\n",
      " [ 0.72449559  0.27550438]\n",
      " [ 0.72449559  0.27550438]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7123614   0.28763857]\n",
      " [ 0.7123614   0.28763857]\n",
      " [ 0.7123614   0.28763857]\n",
      " [ 0.7123614   0.28763857]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70481426  0.29518577]\n",
      " [ 0.70481426  0.29518577]\n",
      " [ 0.70481426  0.29518577]\n",
      " [ 0.70481426  0.29518577]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70035565  0.29964435]\n",
      " [ 0.70035565  0.29964435]\n",
      " [ 0.70035565  0.29964435]\n",
      " [ 0.70035565  0.29964435]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69775909  0.30224088]\n",
      " [ 0.69775909  0.30224088]\n",
      " [ 0.69775909  0.30224088]\n",
      " [ 0.69775909  0.30224088]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.6962449   0.30375504]\n",
      " [ 0.6962449   0.30375504]\n",
      " [ 0.6962449   0.30375504]\n",
      " [ 0.6962449   0.30375504]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69535661  0.30464342]\n",
      " [ 0.69535661  0.30464342]\n",
      " [ 0.69535661  0.30464342]\n",
      " [ 0.69535661  0.30464342]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.6948334  0.3051666]\n",
      " [ 0.6948334  0.3051666]\n",
      " [ 0.6948334  0.3051666]\n",
      " [ 0.6948334  0.3051666]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69452566  0.30547434]\n",
      " [ 0.69452566  0.30547434]\n",
      " [ 0.69452566  0.30547434]\n",
      " [ 0.69452566  0.30547434]] \n",
      "\n",
      "Loss for iteration 42: 1.232330\n",
      "Iteration 0\n",
      "[[ 0.74151456  0.25848547]\n",
      " [ 0.74151456  0.25848547]\n",
      " [ 0.74151456  0.25848547]\n",
      " [ 0.74151456  0.25848547]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72435838  0.27564168]\n",
      " [ 0.72435838  0.27564168]\n",
      " [ 0.72435838  0.27564168]\n",
      " [ 0.72435838  0.27564168]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71218598  0.28781402]\n",
      " [ 0.71218598  0.28781402]\n",
      " [ 0.71218598  0.28781402]\n",
      " [ 0.71218598  0.28781402]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70461053  0.29538953]\n",
      " [ 0.70461053  0.29538953]\n",
      " [ 0.70461053  0.29538953]\n",
      " [ 0.70461053  0.29538953]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.70012707  0.29987296]\n",
      " [ 0.70012707  0.29987296]\n",
      " [ 0.70012707  0.29987296]\n",
      " [ 0.70012707  0.29987296]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.6975078  0.3024922]\n",
      " [ 0.6975078  0.3024922]\n",
      " [ 0.6975078  0.3024922]\n",
      " [ 0.6975078  0.3024922]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69597352  0.30402648]\n",
      " [ 0.69597352  0.30402648]\n",
      " [ 0.69597352  0.30402648]\n",
      " [ 0.69597352  0.30402648]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69506806  0.304932  ]\n",
      " [ 0.69506806  0.304932  ]\n",
      " [ 0.69506806  0.304932  ]\n",
      " [ 0.69506806  0.304932  ]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69453043  0.30546954]\n",
      " [ 0.69453043  0.30546954]\n",
      " [ 0.69453043  0.30546954]\n",
      " [ 0.69453043  0.30546954]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69421065  0.30578935]\n",
      " [ 0.69421065  0.30578935]\n",
      " [ 0.69421065  0.30578935]\n",
      " [ 0.69421065  0.30578935]] \n",
      "\n",
      "Loss for iteration 43: 1.225564\n",
      "Iteration 0\n",
      "[[ 0.74142528  0.25857472]\n",
      " [ 0.74142528  0.25857472]\n",
      " [ 0.74142528  0.25857472]\n",
      " [ 0.74142528  0.25857472]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72423774  0.27576223]\n",
      " [ 0.72423774  0.27576223]\n",
      " [ 0.72423774  0.27576223]\n",
      " [ 0.72423774  0.27576223]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71203125  0.28796875]\n",
      " [ 0.71203125  0.28796875]\n",
      " [ 0.71203125  0.28796875]\n",
      " [ 0.71203125  0.28796875]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70443016  0.29556984]\n",
      " [ 0.70443016  0.29556984]\n",
      " [ 0.70443016  0.29556984]\n",
      " [ 0.70443016  0.29556984]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69992369  0.30007628]\n",
      " [ 0.69992369  0.30007628]\n",
      " [ 0.69992369  0.30007628]\n",
      " [ 0.69992369  0.30007628]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69728303  0.30271694]\n",
      " [ 0.69728303  0.30271694]\n",
      " [ 0.69728303  0.30271694]\n",
      " [ 0.69728303  0.30271694]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69572961  0.30427042]\n",
      " [ 0.69572961  0.30427042]\n",
      " [ 0.69572961  0.30427042]\n",
      " [ 0.69572961  0.30427042]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69480771  0.30519232]\n",
      " [ 0.69480771  0.30519232]\n",
      " [ 0.69480771  0.30519232]\n",
      " [ 0.69480771  0.30519232]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69425631  0.30574369]\n",
      " [ 0.69425631  0.30574369]\n",
      " [ 0.69425631  0.30574369]\n",
      " [ 0.69425631  0.30574369]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69392484  0.30607519]\n",
      " [ 0.69392484  0.30607519]\n",
      " [ 0.69392484  0.30607519]\n",
      " [ 0.69392484  0.30607519]] \n",
      "\n",
      "Loss for iteration 44: 1.224880\n",
      "Iteration 0\n",
      "[[ 0.74134684  0.2586531 ]\n",
      " [ 0.74134684  0.2586531 ]\n",
      " [ 0.74134684  0.2586531 ]\n",
      " [ 0.74134684  0.2586531 ]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72413123  0.27586874]\n",
      " [ 0.72413123  0.27586874]\n",
      " [ 0.72413123  0.27586874]\n",
      " [ 0.72413123  0.27586874]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71189409  0.28810591]\n",
      " [ 0.71189409  0.28810591]\n",
      " [ 0.71189409  0.28810591]\n",
      " [ 0.71189409  0.28810591]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70426983  0.29573017]\n",
      " [ 0.70426983  0.29573017]\n",
      " [ 0.70426983  0.29573017]\n",
      " [ 0.70426983  0.29573017]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69974196  0.30025804]\n",
      " [ 0.69974196  0.30025804]\n",
      " [ 0.69974196  0.30025804]\n",
      " [ 0.69974196  0.30025804]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69708109  0.30291891]\n",
      " [ 0.69708109  0.30291891]\n",
      " [ 0.69708109  0.30291891]\n",
      " [ 0.69708109  0.30291891]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69550937  0.30449063]\n",
      " [ 0.69550937  0.30449063]\n",
      " [ 0.69550937  0.30449063]\n",
      " [ 0.69550937  0.30449063]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69457155  0.30542845]\n",
      " [ 0.69457155  0.30542845]\n",
      " [ 0.69457155  0.30542845]\n",
      " [ 0.69457155  0.30542845]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69400674  0.30599323]\n",
      " [ 0.69400674  0.30599323]\n",
      " [ 0.69400674  0.30599323]\n",
      " [ 0.69400674  0.30599323]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69366401  0.30633599]\n",
      " [ 0.69366401  0.30633599]\n",
      " [ 0.69366401  0.30633599]\n",
      " [ 0.69366401  0.30633599]] \n",
      "\n",
      "Loss for iteration 45: 1.224266\n",
      "Iteration 0\n",
      "[[ 0.74127769  0.25872231]\n",
      " [ 0.74127769  0.25872231]\n",
      " [ 0.74127769  0.25872231]\n",
      " [ 0.74127769  0.25872231]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72403669  0.27596328]\n",
      " [ 0.72403669  0.27596328]\n",
      " [ 0.72403669  0.27596328]\n",
      " [ 0.72403669  0.27596328]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71177202  0.28822792]\n",
      " [ 0.71177202  0.28822792]\n",
      " [ 0.71177202  0.28822792]\n",
      " [ 0.71177202  0.28822792]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.7041266   0.29587343]\n",
      " [ 0.7041266   0.29587343]\n",
      " [ 0.7041266   0.29587343]\n",
      " [ 0.7041266   0.29587343]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69957882  0.30042115]\n",
      " [ 0.69957882  0.30042115]\n",
      " [ 0.69957882  0.30042115]\n",
      " [ 0.69957882  0.30042115]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69689876  0.30310121]\n",
      " [ 0.69689876  0.30310121]\n",
      " [ 0.69689876  0.30310121]\n",
      " [ 0.69689876  0.30310121]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69530952  0.30469054]\n",
      " [ 0.69530952  0.30469054]\n",
      " [ 0.69530952  0.30469054]\n",
      " [ 0.69530952  0.30469054]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69435644  0.30564362]\n",
      " [ 0.69435644  0.30564362]\n",
      " [ 0.69435644  0.30564362]\n",
      " [ 0.69435644  0.30564362]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69377857  0.3062214 ]\n",
      " [ 0.69377857  0.3062214 ]\n",
      " [ 0.69377857  0.3062214 ]\n",
      " [ 0.69377857  0.3062214 ]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69342488  0.30657518]\n",
      " [ 0.69342488  0.30657518]\n",
      " [ 0.69342488  0.30657518]\n",
      " [ 0.69342488  0.30657518]] \n",
      "\n",
      "Loss for iteration 46: 1.223711\n",
      "Iteration 0\n",
      "[[ 0.74121636  0.25878361]\n",
      " [ 0.74121636  0.25878361]\n",
      " [ 0.74121636  0.25878361]\n",
      " [ 0.74121636  0.25878361]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72395247  0.27604756]\n",
      " [ 0.72395247  0.27604756]\n",
      " [ 0.72395247  0.27604756]\n",
      " [ 0.72395247  0.27604756]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71166289  0.28833711]\n",
      " [ 0.71166289  0.28833711]\n",
      " [ 0.71166289  0.28833711]\n",
      " [ 0.71166289  0.28833711]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70399809  0.29600197]\n",
      " [ 0.70399809  0.29600197]\n",
      " [ 0.70399809  0.29600197]\n",
      " [ 0.70399809  0.29600197]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.6994316   0.30056834]\n",
      " [ 0.6994316   0.30056834]\n",
      " [ 0.6994316   0.30056834]\n",
      " [ 0.6994316   0.30056834]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69673342  0.30326664]\n",
      " [ 0.69673342  0.30326664]\n",
      " [ 0.69673342  0.30326664]\n",
      " [ 0.69673342  0.30326664]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69512725  0.30487278]\n",
      " [ 0.69512725  0.30487278]\n",
      " [ 0.69512725  0.30487278]\n",
      " [ 0.69512725  0.30487278]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69415927  0.30584067]\n",
      " [ 0.69415927  0.30584067]\n",
      " [ 0.69415927  0.30584067]\n",
      " [ 0.69415927  0.30584067]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69356889  0.30643114]\n",
      " [ 0.69356889  0.30643114]\n",
      " [ 0.69356889  0.30643114]\n",
      " [ 0.69356889  0.30643114]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69320446  0.30679554]\n",
      " [ 0.69320446  0.30679554]\n",
      " [ 0.69320446  0.30679554]\n",
      " [ 0.69320446  0.30679554]] \n",
      "\n",
      "Loss for iteration 47: 1.223207\n",
      "Iteration 0\n",
      "[[ 0.74116188  0.25883818]\n",
      " [ 0.74116188  0.25883818]\n",
      " [ 0.74116188  0.25883818]\n",
      " [ 0.74116188  0.25883818]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72387701  0.27612299]\n",
      " [ 0.72387701  0.27612299]\n",
      " [ 0.72387701  0.27612299]\n",
      " [ 0.72387701  0.27612299]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71156484  0.28843513]\n",
      " [ 0.71156484  0.28843513]\n",
      " [ 0.71156484  0.28843513]\n",
      " [ 0.71156484  0.28843513]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70388222  0.29611775]\n",
      " [ 0.70388222  0.29611775]\n",
      " [ 0.70388222  0.29611775]\n",
      " [ 0.70388222  0.29611775]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69929832  0.30070171]\n",
      " [ 0.69929832  0.30070171]\n",
      " [ 0.69929832  0.30070171]\n",
      " [ 0.69929832  0.30070171]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69658267  0.30341735]\n",
      " [ 0.69658267  0.30341735]\n",
      " [ 0.69658267  0.30341735]\n",
      " [ 0.69658267  0.30341735]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69496024  0.30503976]\n",
      " [ 0.69496024  0.30503976]\n",
      " [ 0.69496024  0.30503976]\n",
      " [ 0.69496024  0.30503976]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69397801  0.30602193]\n",
      " [ 0.69397801  0.30602193]\n",
      " [ 0.69397801  0.30602193]\n",
      " [ 0.69397801  0.30602193]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69337535  0.30662471]\n",
      " [ 0.69337535  0.30662471]\n",
      " [ 0.69337535  0.30662471]\n",
      " [ 0.69337535  0.30662471]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69300056  0.30699944]\n",
      " [ 0.69300056  0.30699944]\n",
      " [ 0.69300056  0.30699944]\n",
      " [ 0.69300056  0.30699944]] \n",
      "\n",
      "Loss for iteration 48: 1.222748\n",
      "Iteration 0\n",
      "[[ 0.74111319  0.25888684]\n",
      " [ 0.74111319  0.25888684]\n",
      " [ 0.74111319  0.25888684]\n",
      " [ 0.74111319  0.25888684]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72380912  0.27619088]\n",
      " [ 0.72380912  0.27619088]\n",
      " [ 0.72380912  0.27619088]\n",
      " [ 0.72380912  0.27619088]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.7114765   0.28852355]\n",
      " [ 0.7114765   0.28852355]\n",
      " [ 0.7114765   0.28852355]\n",
      " [ 0.7114765   0.28852355]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70377737  0.29622263]\n",
      " [ 0.70377737  0.29622263]\n",
      " [ 0.70377737  0.29622263]\n",
      " [ 0.70377737  0.29622263]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69917691  0.30082306]\n",
      " [ 0.69917691  0.30082306]\n",
      " [ 0.69917691  0.30082306]\n",
      " [ 0.69917691  0.30082306]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69644463  0.3035554 ]\n",
      " [ 0.69644463  0.3035554 ]\n",
      " [ 0.69644463  0.3035554 ]\n",
      " [ 0.69644463  0.3035554 ]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69480658  0.30519342]\n",
      " [ 0.69480658  0.30519342]\n",
      " [ 0.69480658  0.30519342]\n",
      " [ 0.69480658  0.30519342]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.6938104  0.3061896]\n",
      " [ 0.6938104  0.3061896]\n",
      " [ 0.6938104  0.3061896]\n",
      " [ 0.6938104  0.3061896]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69319576  0.30680424]\n",
      " [ 0.69319576  0.30680424]\n",
      " [ 0.69319576  0.30680424]\n",
      " [ 0.69319576  0.30680424]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69281089  0.30718914]\n",
      " [ 0.69281089  0.30718914]\n",
      " [ 0.69281089  0.30718914]\n",
      " [ 0.69281089  0.30718914]] \n",
      "\n",
      "Loss for iteration 49: 1.222327\n",
      "Iteration 0\n",
      "[[ 0.7410695   0.25893053]\n",
      " [ 0.7410695   0.25893053]\n",
      " [ 0.7410695   0.25893053]\n",
      " [ 0.7410695   0.25893053]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.72374779  0.27625221]\n",
      " [ 0.72374779  0.27625221]\n",
      " [ 0.72374779  0.27625221]\n",
      " [ 0.72374779  0.27625221]] \n",
      "\n",
      "Iteration 2\n",
      "[[ 0.71139634  0.28860366]\n",
      " [ 0.71139634  0.28860366]\n",
      " [ 0.71139634  0.28860366]\n",
      " [ 0.71139634  0.28860366]] \n",
      "\n",
      "Iteration 3\n",
      "[[ 0.70368207  0.29631796]\n",
      " [ 0.70368207  0.29631796]\n",
      " [ 0.70368207  0.29631796]\n",
      " [ 0.70368207  0.29631796]] \n",
      "\n",
      "Iteration 4\n",
      "[[ 0.69906592  0.30093405]\n",
      " [ 0.69906592  0.30093405]\n",
      " [ 0.69906592  0.30093405]\n",
      " [ 0.69906592  0.30093405]] \n",
      "\n",
      "Iteration 5\n",
      "[[ 0.69631767  0.30368233]\n",
      " [ 0.69631767  0.30368233]\n",
      " [ 0.69631767  0.30368233]\n",
      " [ 0.69631767  0.30368233]] \n",
      "\n",
      "Iteration 6\n",
      "[[ 0.69466442  0.30533552]\n",
      " [ 0.69466442  0.30533552]\n",
      " [ 0.69466442  0.30533552]\n",
      " [ 0.69466442  0.30533552]] \n",
      "\n",
      "Iteration 7\n",
      "[[ 0.69365472  0.30634525]\n",
      " [ 0.69365472  0.30634525]\n",
      " [ 0.69365472  0.30634525]\n",
      " [ 0.69365472  0.30634525]] \n",
      "\n",
      "Iteration 8\n",
      "[[ 0.69302845  0.30697158]\n",
      " [ 0.69302845  0.30697158]\n",
      " [ 0.69302845  0.30697158]\n",
      " [ 0.69302845  0.30697158]] \n",
      "\n",
      "Iteration 9\n",
      "[[ 0.69263363  0.30736631]\n",
      " [ 0.69263363  0.30736631]\n",
      " [ 0.69263363  0.30736631]\n",
      " [ 0.69263363  0.30736631]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from batcher_gan import DiscriminatorBatcher, GANBatcher\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "               \n",
    "# Generator graph.\n",
    "lr_gen = tf.Variable(0.001, trainable = False)\n",
    "gen_cost, probs = construct_gan()\n",
    "gen_train_op = construct_gan_train_op(gen_cost, lr_gen)              \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in xrange(50):    \n",
    "        _, cost = sess.run([gen_train_op, gen_cost])     \n",
    "        print('Loss for iteration %i: %f' % (i, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.nn import rnn_cell\n",
    "from tensorflow.python.ops.nn import rnn\n",
    "from tensorflow.python.ops.nn import seq2seq\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def construct_discriminator(input_data, targets):\n",
    "    if model == 'rnn':\n",
    "        cell = rnn_cell.BasicRNNCell(rnn_size)\n",
    "    elif model == 'gru':\n",
    "        cell = rnn_cell.GRUCell(rnn_size)\n",
    "    elif model == 'lstm':\n",
    "        cell = rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    else:\n",
    "        raise Exception('model type not supported: {}'.format(model))\n",
    "\n",
    "    cell = rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('rnn'):\n",
    "        softmax_w = tf.get_variable('softmax_w', [rnn_size, 2])\n",
    "        softmax_b = tf.get_variable('softmax_b', [2])\n",
    "        embedding = tf.get_variable('embedding', [vocab_size, rnn_size])\n",
    "        inputs    = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "        inputs    = [tf.squeeze(i, [1]) for i in inputs]\n",
    "\n",
    "        outputs, final_state = seq2seq.rnn_decoder(inputs, initial_state, \n",
    "            cell, loop_function=None)\n",
    "\n",
    "    output_tf = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output_tf, softmax_w, softmax_b)\n",
    "    probs  = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(targets, [-1])],\n",
    "        [tf.ones([batch_size * seq_length])])\n",
    "\n",
    "    cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "\n",
    "    lr          = tf.Variable(0.0, trainable = False)\n",
    "    tvars       = tf.trainable_variables()\n",
    "    grads, _    = tf.clip_by_global_norm(tf.gradients(cost, tvars, aggregation_method=2), grad_clip)\n",
    "    optimizer   = tf.train.AdamOptimizer(lr)\n",
    "    train_op    = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return train_op, cost, lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NLP GAN.\n",
    "Train the NLP GAN by using the two defined networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f5bafc70d90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f5ae0033f90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f5ae0785090>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting reviews at models_GAN/fake_reviews.txt\n",
      "Generating samples to models_GAN/fake_reviews.txt\n",
      "[' ' ' ' ' ' '\\n' ' ' ' ' ' ' ' ' 'b' ' ']\n",
      "[' ' ' ' ' ' ' ' ' ' '\\n' '\\n' ' ' ' ' 'e']\n",
      "[' ' ' ' '\\n' '\\n' ' ' '\\n' ' ' ' ' 'e' ' ']\n",
      "[' ' 'b' '\\n' ' ' 'b' ' ' 'b' '\\n' ' ' ' ']\n",
      "[' ' 'e' ' ' ' ' 'r' ' ' ' ' ' ' ' ' ' ']\n",
      "[' ' ' ' '\\n' ' ' 'r' ' ' ' ' 'b' '\\n' '\\n']\n",
      "[' ' 'b' 'b' ' ' ' ' 'e' ' ' ' ' ' ' 'r']\n",
      "[' ' '\\n' ' ' ' ' ' ' ' ' ' ' '\\n' '\\n' '\\n']\n",
      "[' ' ' ' 'b' 'e' ' ' '\\n' ' ' ' ' '\\n' ' ']\n",
      "[' ' ' ' ' ' ' ' '\\n' ' ' '\\n' ' ' ' ' 'r']\n",
      "Loss for iteration 0: 0.799852\n",
      "Loss for iteration 1: 0.786291\n",
      "Loss for iteration 2: 0.780016\n",
      "Loss for iteration 3: 0.774751\n",
      "Loss for iteration 4: 0.768128\n",
      "Loss for iteration 5: 0.760939\n",
      "Loss for iteration 6: 0.755057\n",
      "Loss for iteration 7: 0.748485\n",
      "Loss for iteration 8: 0.738555\n",
      "Loss for iteration 9: 0.738642\n",
      "Loss for iteration 10: 0.732029\n",
      "Loss for iteration 11: 0.721299\n",
      "Loss for iteration 12: 0.714165\n",
      "Loss for iteration 13: 0.707675\n",
      "Loss for iteration 14: 0.705762\n",
      "Loss for iteration 15: 0.697146\n",
      "Loss for iteration 16: 0.681507\n",
      "Loss for iteration 17: 0.675479\n",
      "Loss for iteration 18: 0.673016\n",
      "Loss for iteration 19: 0.655101\n",
      "Loss for iteration 20: 0.680483\n",
      "Loss for iteration 21: 0.653919\n",
      "Loss for iteration 22: 0.665745\n",
      "Loss for iteration 23: 0.672042\n",
      "Loss for iteration 24: 0.646128\n",
      "Loss for iteration 25: 0.657411\n",
      "Loss for iteration 26: 0.626584\n",
      "Loss for iteration 27: 0.629571\n",
      "Loss for iteration 28: 0.599659\n",
      "Loss for iteration 29: 0.623062\n",
      "Loss for iteration 30: 0.598316\n",
      "Loss for iteration 31: 0.580324\n",
      "Loss for iteration 32: 0.552283\n",
      "Loss for iteration 33: 0.578573\n",
      "Loss for iteration 34: 0.569659\n",
      "Loss for iteration 35: 0.548060\n",
      "Loss for iteration 36: 0.563795\n",
      "Loss for iteration 37: 0.577374\n",
      "Loss for iteration 38: 0.580072\n",
      "Loss for iteration 39: 0.554922\n",
      "Loss for iteration 40: 0.560886\n",
      "Loss for iteration 41: 0.640282\n",
      "Loss for iteration 42: 0.519253\n",
      "Loss for iteration 43: 0.632089\n",
      "Loss for iteration 44: 0.636421\n",
      "Loss for iteration 45: 0.469185\n",
      "Loss for iteration 46: 0.537520\n",
      "Loss for iteration 47: 0.569957\n",
      "Loss for iteration 48: 0.482368\n",
      "Loss for iteration 49: 0.553412\n",
      "Loss for iteration 50: 0.511092\n",
      "Loss for iteration 51: 0.506042\n",
      "Loss for iteration 52: 0.479238\n",
      "Loss for iteration 53: 0.455423\n",
      "Loss for iteration 54: 0.479071\n",
      "Loss for iteration 55: 0.434517\n",
      "Loss for iteration 56: 0.429814\n",
      "Loss for iteration 57: 0.499428\n",
      "Loss for iteration 58: 0.483553\n",
      "Loss for iteration 59: 0.496920\n",
      "Loss for iteration 60: 0.526293\n",
      "Loss for iteration 61: 0.426035\n",
      "Loss for iteration 62: 0.477506\n",
      "Loss for iteration 63: 0.389572\n",
      "Loss for iteration 64: 0.403098\n",
      "Loss for iteration 65: 0.383819\n",
      "Loss for iteration 66: 0.422088\n",
      "Loss for iteration 67: 0.409688\n",
      "Loss for iteration 68: 0.466387\n",
      "Loss for iteration 69: 0.351499\n",
      "Loss for iteration 70: 0.511603\n",
      "Loss for iteration 71: 0.383037\n",
      "Loss for iteration 72: 0.399350\n",
      "Loss for iteration 73: 0.632415\n",
      "Loss for iteration 74: 0.357089\n",
      "Loss for iteration 75: 0.431689\n",
      "Loss for iteration 76: 0.442164\n",
      "Loss for iteration 77: 0.362046\n",
      "Loss for iteration 78: 0.476329\n",
      "Loss for iteration 79: 0.370143\n",
      "Loss for iteration 80: 0.458797\n",
      "Loss for iteration 81: 0.352867\n",
      "Loss for iteration 82: 0.304419\n",
      "Loss for iteration 83: 0.301730\n",
      "Loss for iteration 84: 0.329337\n",
      "Loss for iteration 85: 0.306240\n",
      "Loss for iteration 86: 0.397601\n",
      "Loss for iteration 87: 0.389997\n",
      "Loss for iteration 88: 0.319733\n",
      "Loss for iteration 89: 0.445429\n",
      "Loss for iteration 90: 0.335836\n",
      "Loss for iteration 91: 0.484789\n",
      "Loss for iteration 92: 0.291424\n",
      "Loss for iteration 93: 0.381398\n",
      "Loss for iteration 94: 0.265073\n",
      "Loss for iteration 95: 0.348747\n",
      "Loss for iteration 96: 0.384205\n",
      "Loss for iteration 97: 0.296206\n",
      "Loss for iteration 98: 0.297198\n",
      "Loss for iteration 99: 0.286477\n",
      "Loss for iteration 100: 0.271334\n",
      "Loss for iteration 101: 0.267770\n",
      "Loss for iteration 102: 0.291784\n",
      "Loss for iteration 103: 0.264065\n",
      "Loss for iteration 104: 0.307735\n",
      "Loss for iteration 105: 0.248349\n",
      "Loss for iteration 106: 0.241545\n",
      "Loss for iteration 107: 0.580703\n",
      "Loss for iteration 108: 0.245207\n",
      "Loss for iteration 109: 0.328066\n",
      "Loss for iteration 110: 0.561311\n",
      "Loss for iteration 111: 0.227658\n",
      "Loss for iteration 112: 0.227521\n",
      "Loss for iteration 113: 0.334571\n",
      "Loss for iteration 114: 0.218562\n",
      "Loss for iteration 115: 0.267719\n",
      "Loss for iteration 116: 0.444864\n",
      "Loss for iteration 117: 0.332578\n",
      "Loss for iteration 118: 0.321863\n",
      "Loss for iteration 119: 0.312019\n",
      "Loss for iteration 120: 0.292911\n",
      "Loss for iteration 121: 0.283126\n",
      "Loss for iteration 122: 0.277045\n",
      "Loss for iteration 123: 0.305913\n",
      "Loss for iteration 124: 0.270609\n",
      "Loss for iteration 125: 0.258248\n",
      "Loss for iteration 126: 0.315753\n",
      "Loss for iteration 127: 0.285179\n",
      "Loss for iteration 128: 0.444402\n",
      "Loss for iteration 129: 0.216280\n",
      "Loss for iteration 130: 0.231242\n",
      "Loss for iteration 131: 0.222883\n",
      "Loss for iteration 132: 0.305789\n",
      "Loss for iteration 133: 0.285027\n",
      "Loss for iteration 134: 0.360622\n",
      "Loss for iteration 135: 0.394934\n",
      "Loss for iteration 136: 0.335384\n",
      "Loss for iteration 137: 0.296711\n",
      "Loss for iteration 138: 0.236618\n",
      "Loss for iteration 139: 0.483247\n",
      "Loss for iteration 140: 0.291150\n",
      "Loss for iteration 141: 0.333492\n",
      "Loss for iteration 142: 0.327482\n",
      "Loss for iteration 143: 0.225792\n",
      "Loss for iteration 144: 0.450290\n",
      "Loss for iteration 145: 0.302954\n",
      "Loss for iteration 146: 0.281824\n",
      "Loss for iteration 147: 0.254101\n",
      "Loss for iteration 148: 0.256622\n",
      "Loss for iteration 149: 0.233049\n",
      "Loss for iteration 150: 0.270773\n",
      "Loss for iteration 151: 0.236740\n",
      "Loss for iteration 152: 0.345137\n",
      "Loss for iteration 153: 0.395152\n",
      "Loss for iteration 154: 0.349715\n",
      "Loss for iteration 155: 0.281100\n",
      "Loss for iteration 156: 0.305541\n",
      "Loss for iteration 157: 0.391389\n",
      "Loss for iteration 158: 0.230520\n",
      "Loss for iteration 159: 0.246629\n",
      "Loss for iteration 160: 0.236093\n",
      "Loss for iteration 161: 0.297571\n",
      "Loss for iteration 162: 0.242625\n",
      "Loss for iteration 163: 0.256606\n",
      "Loss for iteration 164: 0.455435\n",
      "Loss for iteration 165: 0.244672\n",
      "Loss for iteration 166: 0.225130\n",
      "Loss for iteration 167: 0.423514\n",
      "Loss for iteration 168: 0.239536\n",
      "Loss for iteration 169: 0.220293\n",
      "Loss for iteration 170: 0.275949\n",
      "Loss for iteration 171: 0.253900\n",
      "Loss for iteration 172: 0.204686\n",
      "Loss for iteration 173: 0.523752\n",
      "Loss for iteration 174: 0.260927\n",
      "Loss for iteration 175: 0.324140\n",
      "Loss for iteration 176: 0.378213\n",
      "Loss for iteration 177: 0.283725\n",
      "Loss for iteration 178: 0.254260\n",
      "Loss for iteration 179: 0.300658\n",
      "Loss for iteration 180: 0.288161\n",
      "Loss for iteration 181: 0.247158\n",
      "Loss for iteration 182: 0.390397\n",
      "Loss for iteration 183: 0.248122\n",
      "Loss for iteration 184: 0.225645\n",
      "Loss for iteration 185: 0.240240\n",
      "Loss for iteration 186: 0.493869\n",
      "Loss for iteration 187: 0.289680\n",
      "Loss for iteration 188: 0.236574\n",
      "Loss for iteration 189: 0.362712\n",
      "Loss for iteration 190: 0.313225\n",
      "Loss for iteration 191: 0.279350\n",
      "Loss for iteration 192: 0.424424\n",
      "Loss for iteration 193: 0.306880\n",
      "Loss for iteration 194: 0.319794\n",
      "Loss for iteration 195: 0.228387\n",
      "Loss for iteration 196: 0.227830\n",
      "Loss for iteration 197: 0.242186\n",
      "Loss for iteration 198: 0.272917\n",
      "Loss for iteration 199: 0.246454\n"
     ]
    }
   ],
   "source": [
    "from batcher_gan import DiscriminatorBatcher, GANBatcher\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# def reset_reviews(data_dir, file_name):\n",
    "#     '''Clear the file containing the generated reviews.\n",
    "    \n",
    "#     Args:\n",
    "#         data_dir:  Directory to store generated reviews.\n",
    "#         file_name:  Name of file containing generated reviews.\n",
    "#     '''\n",
    "#     print('Reseting reviews at %s' % os.path.join(data_dir, file_name))\n",
    "#     open(os.path.join(data_dir, file_name), 'w').close()\n",
    "   \n",
    "\n",
    "\n",
    "def generate_samples(sess, save_dir, data_file, sample_op, chars):\n",
    "    '''Generate samples.'''\n",
    "    data_file = os.path.join(save_dir, data_file)\n",
    "    indices = sess.run(sample_op)\n",
    "    int_to_char = lambda x: chars[x]\n",
    "    mapfunc = np.vectorize(int_to_char)\n",
    "    samples = mapfunc(indices.T)\n",
    "    print('Generating samples to %s' % data_file)    \n",
    "    with open(data_file, 'a+') as f:\n",
    "        for line in samples:\n",
    "            print line\n",
    "            print>>f, ''.join(line) \n",
    "                  \n",
    "\n",
    "# Generator graph.\n",
    "gen_train_op, sample_op = construct_gan()\n",
    "\n",
    "# Discriminator graph.\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "dis_train_op, dis_cost, lr = construct_discriminator(input_data, targets)\n",
    "                            \n",
    "with tf.Session() as sess:\n",
    "    # Graph initialization.\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "        \n",
    "    # Clear old samples.\n",
    "    reset_reviews(save_dir_GAN, fake_input_file)\n",
    "    \n",
    "    # Generate new samples.\n",
    "    chars, vocab = load_vocab(save_dir_GAN, vocab_file)\n",
    "    generate_samples(sess, save_dir_GAN, fake_input_file, sample_op, chars)\n",
    "    batcher  = DiscriminatorBatcher(real_input_file, \n",
    "                                        fake_input_file, \n",
    "                                        data_dir, vocab_file,\n",
    "                                        batch_size, seq_length)\n",
    "    batcher.reset_batch_pointer()\n",
    "    \n",
    "    # Training Discriminator.\n",
    "    for i in xrange(200):\n",
    "        x, y  = batcher.next_batch()\n",
    "        \n",
    "        # TODO: Why use assign vs. a feed?\n",
    "        # Best approach for this?\n",
    "#         sess.run(tf.assign(lr, dis_lr))\n",
    "        feed = {input_data: x, targets: y, lr: dis_lr}\n",
    "        cost, _ = sess.run([dis_cost, dis_train_op], feed)\n",
    "        \n",
    "        print('Loss for iteration %i: %f' % (i, cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[[-0.47268498 -0.29608265 -0.6129449   0.68132377  0.20629463]\n",
      " [-0.47268498 -0.29608265 -0.6129449   0.68132377  0.20629463]] \n",
      "\n",
      "Iteration 1\n",
      "[[ 0.13426088 -0.31010672 -0.1337792  -0.00184738  0.48531109]\n",
      " [-0.2791602  -0.23040935 -0.2588793   0.36823389  0.14612493]] \n",
      "\n",
      "Iteration 2\n",
      "[[-0.46070901 -0.16181867 -0.94683945  0.83026254  0.16507755]\n",
      " [-0.37143287 -0.02318827  0.17404538 -0.07132882 -0.11844563]] \n",
      "\n",
      "Iteration 3\n",
      "[[-0.46001613 -0.20393686 -0.96491438  0.79266554  0.1573296 ]\n",
      " [-0.40352771  0.12216403  0.66109788 -0.24784262  0.14009601]] \n",
      "\n",
      "Iteration 0\n",
      "['e', 'e'] \n",
      "\n",
      "Iteration 1\n",
      "['b', '\\n'] \n",
      "\n",
      "Iteration 2\n",
      "['\\n', 'b'] \n",
      "\n",
      "Iteration 3\n",
      "[' ', ' '] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cPickle\n",
    "from tensorflow.contrib.distributions import Categorical\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def map_to_text(indices, chars):\n",
    "    '''Return text from indices'''\n",
    "    text = []\n",
    "    for index in indices:\n",
    "        text.append(chars[index])\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate():    \n",
    "    # Initial indices\n",
    "    batch_indices = tf.constant(0, shape=[batch_size])\n",
    "\n",
    "    # RNN\n",
    "    outputs, indices = [], []\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(rnn_size)\n",
    "    state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "    with tf.variable_scope('rnn'):\n",
    "        # Embeddings and Logits\n",
    "        embedding = tf.get_variable('embedding', [vocab_size, rnn_size])\n",
    "        softmax   = tf.get_variable('softmax', [rnn_size, vocab_size])\n",
    "\n",
    "        inp = tf.nn.embedding_lookup(embedding, batch_indices)\n",
    "        for i in xrange(seq_length):\n",
    "            indices.append(batch_indices)\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            rnn_out, state = cell(inp, state)\n",
    "            logits_out = tf.matmul(rnn_out, softmax)\n",
    "            outputs.append(logits_out)\n",
    "            batch_indices = tf.squeeze(Categorical(logits_out).sample(n=1))\n",
    "            inp = tf.nn.embedding_lookup(embedding, batch_indices)\n",
    "  \n",
    "    return outputs, indices\n",
    "            \n",
    "\n",
    "with tf.Session() as sess: \n",
    "    outputs, indices = generate()\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    chars, vocab = load_vocab(save_dir_GAN, vocab_file)\n",
    "    \n",
    "    for i, output in enumerate(outputs):\n",
    "        print 'Iteration %d'%i\n",
    "        print sess.run(output),'\\n'\n",
    "    \n",
    "    for i, batch_indices in enumerate(indices):\n",
    "        print 'Iteration %d'%i\n",
    "        indices_eval = sess.run(batch_indices)\n",
    "        print map_to_text(indices_eval, chars),'\\n'\n",
    "    \n",
    "#     for line in indices:\n",
    "#         print ''.join(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
